// UNCLASSIFIED

extends site
append site_help
	:markdown
		See also the companion [skinning guide](/skinguide.jade) and the [programmers ref manual](/shares/doc/totem/index.html).
append site_parms
	- view = "Typical"
	- dock = "left"
append site_body

	#fit.Intro

		:markdown
			TOTEM provides CRUD endpoints to synchronize dataset NODES:

				(select) GET 	 /NODE $$ NODE ...
				(update) PUT 	 /NODE $$ NODE ...
				(insert) POST 	 /NODE $$ NODE ...
				(delete) DELETE /NODE $$ NODE ...

			where a NODE:

				DATASET.TYPE ? PARMS
				ENGINE.TYPE ? PARMS
				AREA/PATH.TYPE ? PARMS
				FILE.TYPE ? PARMS

			addresses a database **dataset**, a compute **engine**, a static area/path **file**, or a parsable **file**.
			The dataset TYPE:

				db | xml | csv | txt | tab | view | tree | flat | delta | nav | kml | encap | html | 
				view | run | plugni | pivot | site | spivot | brief | gridbrief | pivbrief | runbrief
				exe |
				code | jade | classif | readability | client | size | risk

			specifies how the dataset is formatted, rendered, queued, or attribute returned.


	#accordion.Views

		#fit.Endpoints

			:markdown
				Views are [editable](/edit.view) [Jade skinning engines](/skinguide.view) that defines 
				a client's view, and are accessed at the following routes:

					GET	/SKIN.view	Render SKIN from enabled engines or internal jade files
					GET	/DATASET.view	Dynamically generate a skin for this DATASET

		#fit.Parameters
			:markdown
				Views parameters are SKIN dependent. For example, [plot.view](/plot.view) with no
				parameters will generate help for its parameters.

		#fit.Examples
			:markdown
				[view plot help](/plot.view)  
				[view the intake dataset using this skin](/intake.view)  
				[view the skinning guide](/skinguide.view)  
				[view the api](/api.view)  
				[view a sample application](/swag.view)  
				[view models](/model.view)  
				[view a sample briefing](/home_brief.view)  
				[view briefing under the ELT1 area](/ELT1.home_brief.view)

	#accordion.Files
		#fit.Endpoints
			:markdown
				File access is provided on the following endpoints:

					GET	/AREA/FILE.TYPE ? PARMS	Return FILE from AREA using query parameters
					GET	/ATTR/FILE.TYPE ? PARMS	Return ATTRribute of a FILE
					GET	/AREA/	Return list of files in this AREA

				where AREA references a #{nick} internal file store or a one of its virtual stores:

					code	file pulled from the [engines db](/engine.view)
					jade	file pulled from the [engines db](/engine.view) or (failing that) the jade file area
					uploads	file pulled from delete-on-access area
					stores	file pulled from long-term area
					positives	file pulled from positive-proof area
					negatives	file pulled from negative-proof area
					cert	generates a PKI cert for the requested user
					chips
					tips
					shares

		#fit.Parameters
			:markdown
				File retrieval parameters include:

					_has	find best file by string containment 
					_nlp	find best file by nlp context 
					_bin	find best file by binary expression 
					_qex	find best file by query expansion 
					_score	minimum score required

		#folder.Readers
			#fit.Intro
				:markdown
					Readers are builtin [engines](/engine.view) that automatically index a variety of 
					document, graphics, presentation, and spreadsheet files when uploaded into
					**#{nick}**.  Ingested text is checked for readibility, indexed to the best
					using [NLP training rules](/admins.view), then reflected into the [file stores](/files.view).

			#fit.Special
				code.
					html	- Web site
					rss	- News feed
					idop	- NTM imagery

			#fit.Document
				code.
					bib      - BibTeX [.bib]
					doc      - Microsoft Word 97/2000/XP [.doc]
					doc6     - Microsoft Word 6.0 [.doc]
					doc95    - Microsoft Word 95 [.doc]
					docbook  - DocBook [.xml]
					docx     - Microsoft Office Open XML [.docx]
					docx7    - Microsoft Office Open XML [.docx]
					fodt     - OpenDocument Text (Flat XML) [.fodt]
					html     - HTML Document (OpenOffice.org Writer) [.html]
					latex    - LaTeX 2e [.ltx]
					mediawiki - MediaWiki [.txt]
					odt      - ODF Text Document [.odt]
					ooxml    - Microsoft Office Open XML [.xml]
					ott      - Open Document Text [.ott]
					pdb      - AportisDoc (Palm) [.pdb]
					pdf      - Portable Document Format [.pdf]
					psw      - Pocket Word [.psw]
					rtf      - Rich Text Format [.rtf]
					sdw      - StarWriter 5.0 [.sdw]
					sdw4     - StarWriter 4.0 [.sdw]
					sdw3     - StarWriter 3.0 [.sdw]
					stw      - Open Office.org 1.0 Text Document Template [.stw]
					sxw      - Open Office.org 1.0 Text Document [.sxw]
					text     - Text Encoded [.txt]
					txt      - Text [.txt]
					uot      - Unified Office Format text [.uot]
					vor      - StarWriter 5.0 Template [.vor]
					vor4     - StarWriter 4.0 Template [.vor]
					vor3     - StarWriter 3.0 Template [.vor]
					xhtml    - XHTML Document [.html]

			#fit.Graphics
				code.
					bmp      - Windows Bitmap [.bmp]
					emf      - Enhanced Metafile [.emf]
					eps      - Encapsulated PostScript [.eps]
					fodg     - OpenDocument Drawing (Flat XML) [.fodg]
					gif      - Graphics Interchange Format [.gif]
					html     - HTML Document (OpenOffice.org Draw) [.html]
					jpg      - Joint Photographic Experts Group [.jpg]
					met      - OS/2 Metafile [.met]
					odd      - OpenDocument Drawing [.odd]
					otg      - OpenDocument Drawing Template [.otg]
					pbm      - Portable Bitmap [.pbm]
					pct      - Mac Pict [.pct]
					pdf      - Portable Document Format [.pdf]
					pgm      - Portable Graymap [.pgm]
					png      - Portable Network Graphic [.png]
					ppm      - Portable Pixelmap [.ppm]
					ras      - Sun Raster Image [.ras]
					std      - OpenOffice.org 1.0 Drawing Template [.std]
					svg      - Scalable Vector Graphics [.svg]
					svm      - StarView Metafile [.svm]
					swf      - Macromedia Flash (SWF) [.swf]
					sxd      - OpenOffice.org 1.0 Drawing [.sxd]
					sxd3     - StarDraw 3.0 [.sxd]
					sxd5     - StarDraw 5.0 [.sxd]
					sxw      - StarOffice XML (Draw) [.sxw]
					tiff     - Tagged Image File Format [.tiff]
					vor      - StarDraw 5.0 Template [.vor]
					vor3     - StarDraw 3.0 Template [.vor]
					wmf      - Windows Metafile [.wmf]
					xhtml    - XHTML [.xhtml]
					xpm      - X PixMap [.xpm]

			#fit.Presentation
				code.
					bmp      - Windows Bitmap [.bmp]
					emf      - Enhanced Metafile [.emf]
					eps      - Encapsulated PostScript [.eps]
					fodp     - OpenDocument Presentation (Flat XML) [.fodp]
					gif      - Graphics Interchange Format [.gif]
					html     - HTML Document (OpenOffice.org Impress) [.html]
					jpg      - Joint Photographic Experts Group [.jpg]
					met      - OS/2 Metafile [.met]
					odg      - ODF Drawing (Impress) [.odg]
					odp      - ODF Presentation [.odp]
					otp      - ODF Presentation Template [.otp]
					pbm      - Portable Bitmap [.pbm]
					pct      - Mac Pict [.pct]
					pdf      - Portable Document Format [.pdf]
					pgm      - Portable Graymap [.pgm]
					png      - Portable Network Graphic [.png]
					potm     - Microsoft PowerPoint 2007/2010 XML Template [.potm]
					pot      - Microsoft PowerPoint 97/2000/XP Template [.pot]
					ppm      - Portable Pixelmap [.ppm]
					pptx     - Microsoft PowerPoint 2007/2010 XML [.pptx]
					pps      - Microsoft PowerPoint 97/2000/XP (Autoplay) [.pps]
					ppt      - Microsoft PowerPoint 97/2000/XP [.ppt]
					pwp      - PlaceWare [.pwp]
					ras      - Sun Raster Image [.ras]
					sda      - StarDraw 5.0 (OpenOffice.org Impress) [.sda]
					sdd      - StarImpress 5.0 [.sdd]
					sdd3     - StarDraw 3.0 (OpenOffice.org Impress) [.sdd]
					sdd4     - StarImpress 4.0 [.sdd]
					sxd      - OpenOffice.org 1.0 Drawing (OpenOffice.org Impress) [.sxd]
					sti      - OpenOffice.org 1.0 Presentation Template [.sti]
					svg      - Scalable Vector Graphics [.svg]
					svm      - StarView Metafile [.svm]
					swf      - Macromedia Flash (SWF) [.swf]
					sxi      - OpenOffice.org 1.0 Presentation [.sxi]
					tiff     - Tagged Image File Format [.tiff]
					uop      - Unified Office Format presentation [.uop]
					vor      - StarImpress 5.0 Template [.vor]
					vor3     - StarDraw 3.0 Template (OpenOffice.org Impress) [.vor]
					vor4     - StarImpress 4.0 Template [.vor]
					vor5     - StarDraw 5.0 Template (OpenOffice.org Impress) [.vor]
					wmf      - Windows Metafile [.wmf]
					xhtml    - XHTML [.xml]
					xpm      - X PixMap [.xpm]

			#fit.Spreadsheet
				code.
					csv      - Text CSV [.csv]
					dbf      - dBASE [.dbf]
					dif      - Data Interchange Format [.dif]
					fods     - OpenDocument Spreadsheet (Flat XML) [.fods]
					html     - HTML Document (OpenOffice.org Calc) [.html]
					ods      - ODF Spreadsheet [.ods]
					ooxml    - Microsoft Excel 2003 XML [.xml]
					ots      - ODF Spreadsheet Template [.ots]
					pdf      - Portable Document Format [.pdf]
					pxl      - Pocket Excel [.pxl]
					sdc      - StarCalc 5.0 [.sdc]
					sdc4     - StarCalc 4.0 [.sdc]
					sdc3     - StarCalc 3.0 [.sdc]
					slk      - SYLK [.slk]
					stc      - OpenOffice.org 1.0 Spreadsheet Template [.stc]
					sxc      - OpenOffice.org 1.0 Spreadsheet [.sxc]
					uos      - Unified Office Format spreadsheet [.uos]
					vor3     - StarCalc 3.0 Template [.vor]
					vor4     - StarCalc 4.0 Template [.vor]
					vor      - StarCalc 5.0 Template [.vor]
					xhtml    - XHTML [.xhtml]
					xls      - Microsoft Excel 97/2000/XP [.xls]
					xls5     - Microsoft Excel 5.0 [.xls]
					xls95    - Microsoft Excel 95 [.xls]
					xlt      - Microsoft Excel 97/2000/XP Template [.xlt]
					xlt5     - Microsoft Excel 5.0 Template [.xlt]
					xlt95    - Microsoft Excel 95 Template [.xlt]
				
		#fit.Examples
			:markdown
				[download a file from the shares area](/shares/welcome.pdf)  
				[return flare json file from data area](/data/flare.json)

	#accordion.Datasets
		#fit.Endpoints
			:markdown
				Both real and virtual datasets are reached at the following endpoints:

					GET	/DATASET.TYPE		Return data from DATASET
					PUT	/DATASET.TYPE		Update DATASET with body parameters
					POST	/DATASET.TYPE	Insert body parameters into DATASET
					DELETE	/DATASET.TYPE	Delete from DATASET 

		#fit.Parameters
			:markdown
				The #{nick} recognizes the following parameters.

				# Query parameters

					KEY = value 		// matching test
					KEY = [ ] 				// null test
					KEY = [ min , max ] // inclusive range (if unsafe queries enabled)
					KEY = [ x, y, z, ... ] // bounding box (reserved)
					A.KEY = B.KEY 	// Join datasets A and B on specified KEYs (experimental)
					expression 			// e.g. KEY<value, KEY>value,... (if unsafe queries enabled)

				# Body and File Upload parameters

				JSON formatted parameters, or a file upload script following this convention:

					key = val ; key = val ; ...  [cr newline]
					:
					:
					file data [cr newline]

				# Searching parameters

					_has	= search records by containment over fulltext keys
					_nlp	= search records by nlp context over fulltext keys
					_bin	= search records by binary expression over fulltext keys
					_qex	= search records by query expansion over fulltext keys
					_score	= minimum search score required
					_save = name to save current url

				# Indexing and Grouping parameters

					_pivot	= KEY,KEY,... pivot records on KEYs with NodeID = "ID,ID,..." groups
					_browse	= KEY,KEY,... browse records on KEYs with NodeID = "name/name/ ..." 
					_group	= KEY,KEY,... group records on KEYs
					_index	= KEY,KEY,... return only specified KEYs
					_sort	= KEY,KEY,... sort records ascending on KEYs
					_sort	= [{property:KEY,sort:DIRECT}, ...] sort records asc|desc on KEYs 
					_geo = KEY,KEY,... add geojson geometry keys named gKEY to the records

				# Conversion parameters

					_queue	= KEY for queuing record state
					_json	= KEY,KEY,... json parse selected KEYs and index with KEY=IDX.IDX...
					_blog	= KEY,KEY,... markdown KEYS with followed [image | post | update | LABEL,W,H](url) tags
					_nav	= open | tree | rename | size ,root/A/B/... folder navigation 

				# Record parameters

					_client	= name of client to broadcast to other clients
					_lock	= enable record locking
					_view 	= name of view to correlate with dataset
					_src	= override dataset path 
					_limit	= number of records to return
					_start	= record position to start returning records

				# Legacy parameter
					_page	= page number (reserved)
					_mark	= KEY,KEY,... markdown these KEYs 
					_jade	= KEY,KEY,... jadeify these KEYs (experimental - use _kjade)
					_kjade	= KEY,KEY,... jadeify these KEYs (to be retired)

		#fit.Examples
			:markdown
				[return first 20 test records having x=123 and y is null sorted by u and b](/test.db?_start=0&_limit=10&x=123&y=null)   
				[return test records having x=123 and order by u and v](/test.db?sort=[{"property":"u","direction":"asc"},{"property":"v","direction":"asc"}]&x=123)   
				[return parms records](/parms.db)  
				[return parms records having specified parm](/parms.db?Parm=Band)  
				[return news records within certain age range](/news.db?age=690:693)  					
				[insert test record x=123,y=null](POST/test.db?x=123&y=null})  
				[update test record ID=10 with x=123,y=null ](PUT/test.db?x=123&y=null&ID=10)  
				[delete test record ID=10](DELETE/test.db?ID:10)  
				[return intake records](/intake.db)  
				[return intake records pivoted by TRL and Ver](/intake.tree?_group=TRL,VER)

		#fit.Virtual.Datasets
			#grid.DS.Parms(
				path="/parms.db",crush,
				head="Print,Help",
				cols="ID.a,Parm,Label,Type,Special,By Inspect.c,By Analysis.c,By Demo.c")

				:markdown
					The [parameter list](/parms.db) dataset defines how dataset fields are exposed to end 
					clients in grids, forms, folders, etc used in **#{nick}** skins.  To each field corresponds a label name, verification 
					methods, and access priviledges.

			#grid.Roles(
				path="/roles.db",crush,
				head="Print,Help",
				cols="Table.T,Special.H,INSERT.T,UPDATE.T,DELETE.T,SELECT.T,IMPORT.T,EXPORT.T")

				:markdown
					The [user roles](/roles.db) dataset defines the roles assumed when clients insert, update, delete, and 
					select records from a specific dataset. 

			#grid.Intrinsic(
				path="/TABLES.db",crush,
				head="Print,Help",
				cols="Name.h")

				:markdown
					The [TABLES](/TABLES.db) provides a list of **#{nick}** datasets.

			#grid.Admin(
				path="/ADMIN.db",crush,
				head="Print,Help",
				cols="TABLE_NAME,TABLE_TYPE,ENGINE,VERSION,ROW_FORMAT,TABLE_ROWS,AVG_ROW_LENGTH,DATA_LENGTH,MAX_DATA_LENGTH,CREATE_TIME,UPDATE_TIME,TABLE_COMMENT")

				:markdown
					The [ADMIN](/ADMIN.db) provides detailed storage and technical information on all **#{nick}** datasets.

			#grid.Sys.Config(
				path="/CONFIG.db",crush,
				head="Print,Help",
				cols="classif,extnet,disk,cpu,cpuat,platform,totalmem,freemem,uptime,cpus,host,netif,temp")

				:markdown
					System configuration information is available at [CONFIG](/CONFIG.db).

			:markdown
				The **#{nick}** supports *Virtual Datasets* through its CRUDE (*create*, *read*, *update*, *delete*, 
				and *execute*) interface.  This CRUDE interface governs both *Database Datasets* and *Virtual Datasets*.
				For example, an *execute* on dataset X will typically import/export the data to/from dataset 
				X as controlled by its associated query parameters, a list of which might be returned by supplying a 
				&help parameter.  Below are several important **Virtual datasets**:

				+	[return upload files](/uploads.FILES.db), [stores files](/stores.FILES.db), etc
				+	[return jade views](/VIEWS.db)  
				+	[return connected users](/USERS.db)  
				+	[return engine summary](/ENGINES.db)  
				+	[return queue summary](/QUEUES.db)  
				+	[return work cliques](/CLIQUES.db)  
				+	[return system health](/HEALTH.db)  
				+	[return database activity](/ACTIVITY.db)  
				+	[return system configuration](/CONFIG.db)  
				+	[flatten searchable tables](/CATALOG.execute) for [search catalog](/CATALOG.db)  
				+	[contengency data](/ROCSUM.db)  
				+	[update](/events.execute) work predication [events](/events.db) and [stats](/jobstats.db)  
				+	[reflect git change logs](/issues.execute) to [tracked issues](/issues.db)  
				+	[broadcast messages](/sockets.execute) to [connected users](/sockets.db)  
				+	[import milestones](/milestone.execute) from internal spreadsheet

	#accordion.Plugins

		:markdown
			A plugin -- a dataset-engine pair -- is assessible from the following endpoints:
			
					GET /PLUGIN.exe?ID=CASE 	Run PLUGIN engine using PLUGIN dataset parameters for specified use CASE 
					GET /PLUGIN.exe?Name=CASE 	Run PLUGIN engine using PLUGIN dataset parameters for specified use CASE
			
			The following (reserved) dataset parameters determine how a PLUGIN operates from an
			exe-endpoint:
			
				(Save) json reserve for storing plugin results for this CASE
				(Job) json  parameters to place CASE into QoS-Credit regulated queue.  Ingest will clone CASE="ingest" records.
				(Agent) name of agent to out-source this CASE which will be polled until it returns its results 
				(Task) name of project associated with this CASE
				(Description) bloggable report on this CASE				
				(Ingest) switch to ingest results produced by this CASE
				
			### Plugin dataset keys
			Keys can be added to or dropped from a plugin via:
			
				GET /PLUGIN.add?KEY=VALUE&... 	Add KEYs to the PLUGIN that can accomodate VALUEs
				GET /PLUGIN.sub?KEY&... 	Drop KEYs from the PLUGIN
			
			### Detecting jobs
			A Job = {ring: ... } will attach the requested PLUGIN to a chipping workflow with the following
			parameters:

				size = 50  	// feature size in [m]
				pixels = 512 	// samples across a chip [pixels]
				scale = 8  		// scale^2 is max number of features in a chip
				step = 0.01 	// relative seach step size
				range = 0.1 	// relative search size
				detects = 8		// hits required to declare a detect
				limit = 1e99 	// restrict maximum number of schips to ingest
				test = "test" 	// test case to store results

			### Counting jobs
			Reserved
			
			### Tagging jobs
			Reserved
			
			### Agent workflow
			An engine can be outsourced to agents at:

				GET /ENGINE.exe?agent=AGENT&poll=N&QUERY
				GET /ENGINE.exe?agent=AGENT&QUERY

			where the &poll request will start a job, then poll the AGENT every N seconds for its results.
			Conversely, a poll-less request will start a job, then reply on the AGENT to claim the job.  In
			either case, QUERY defines the ENGINE args.

			#{title} reciprocates by providing its own AGENT at its **/agent** endpoint.

			Agents provide a means to outsource an engine, while retaining a thread
			on each agent request.  A valid agent provided #{title} with the
			following push/pull endpoints:

				GET http://AGENT?push=totem.CLIENT.ENGINE.ID&args=JSON
				GET http://AGENT?pull=JOBID

			where the push endpoint defines the job being sent to the AGENT (with the CLIENT 
			requesting the agent, the ENGINE being outsourced, the ID of the test case,
			and the JSON args to the ENGINE).  The AGENT eventually responds at 
			the pull endpoint with a JOBID, or a "" if no job could be created.  #{title} will periodically 
			poll the agent for the results of JOBID.

	#accordion.Engines

		#fit.Endpoints
			:markdown
				[Engines](/engine.view) are built on cv | py | R | sh | js | res1 | res2 | ... machines and are
				accessible from the following endpoints:

					GET /ENGINE.TYPE 	Compile and step ENGINE in a stateless workflow
					PUT /ENGINE.TYPE 	Compile ENGINE in a stateful workflow
					POST /ENGINE.TYPE	Step ENGINE  in a stateful workflow
					DELETE /ENGINE.TYPE	Free ENGINE from a stateful workflow

				were TYPE = db | txt | view | html | .... specifies how the engine's *context* is delivered to the client
				(TYPE **does not** imply anything about the engine's machine type).  A TYPE = exe will execute
				the engine and return its results.  Please know 
				that **#{nick}** does not provide an interactive development environment; thus engines should be 
				thoroughly debugged in their native development environment before being [inserted](/engine.view) 
				into **#{nick}**.  *Engines* become *plugins* when they are paired with a same-named dataset.

				While this endpoint:
				
					GET /ENGINE.TYPE 
					
				provides a stateless interface to an engine and its context, this endpoints:
				
					PUT /ENGINE.TYPE 
					POST /ENGINE.TYPE	
					DELETE /ENGINE.TYPE	

				provide a stateful interface to engines which are useful in [workflows](/nodeflow.view).  
				
				Optional engine parameters are held in a read/write context that follows the pattern:

					{
						tau: [ tau1, tau2, ... ],
						port1: function () {}, 
						port2: function() {}, ...
						entry: {key1: "select ...", ... } || "select ...",
						exit: {key2: "update ...", ... } || "update ...",
						key: value, 
						urlKey: value
					}
					
				When an engine is accessed, its context is automatically extended with the following modules:

					[DSP](https://www.npmjs.com/package/dsp) ,
					[LWIP](https://github.com/EyalAr/lwip) ,  
					[MATH](http://mathjs.org/) ,  
					[CRYPTO](https://nodejs.org/docs/latest/api/) ,   
					[FLEX](https://github.com/ACMESDS/flex),
					[SVD)(https://www.npmjs.com/package/node-svd)
					
				FLEX contains several (ctx, res)-methods including:

					FLEX.get( {ds: "...", where: {key:value, ...}}, (data) => {...} )  
					FLEX.randpr( { jumprates, ... }, (data) => {...} )    // generate markov random process
					FLEX.gaussmix( { events: [ {x,y,z,t ...}, ...], scenario: {}, ... }, (data) => {...} )  // generate (mean,sigma) estimates
					FLEX.wms( { ... }, (data) => {...} )  // wms response
					FLEX.wfs( { ... }, (data) => {...} )  // wfs response
					FLEX.sss( { ... }, (data) => {...} )  // sss response
					
				Whereas stateless engines (being memoryless) are initialized at each step, stateful 
				engines are initialized only when the workflow is initialized.  When initialized, an engine's context 
				defines its i/o ports *nameX*, its default i/o event tokens *tau*, and its *url key:value* 
				parameters.  An engine's context variables *varX* are loaded (and stored) 
				from (to) its underlying sql database using *"select ..."* (an *"update ..."*) queries on
				engine entry (and exit).

				If used, each *tau* event can contain anything, for example:

					job = "" 	# Fully qualified file path to a jpg chip for ingesting
					work = 0 	# Anticipated/delivered data volume (dims bits etc)
					disem = "" 	# Disemination channel for this event
					classif = "" # Classification of this event
					cost = ""	# Billing center
					policy = ""	# Data retention policy
					status = 0	# Status code
					value = 0	# Flow calculation

		#folder.Examples
			#fit.url
				:markdown
					URL engines (experimental) provide a simple method to re-route a request
					based on URL parameters, ${req.PARM} and ${plugin.FN(req)} tags; such engines
					follow the pattern:

						Code:
							"http://URL?PARM=${req.PARM}&PARM=${plugin.FN(req)}"
						Context:
							{ fn1: function (req) {...}, fn2: ... }

					For example, a **testeng** engine:

						Code:
							"http://www.someservice.com/ogc?service=wfs&layer=${req.id+"xxx"}&arg=${plugin.angproj(req)}"
						Context:
							{ angproj: function (req) { 
								return Math.cos(req.arg*Math.PI/180); 
								} }

					would reroute "/testeng.xml?id=abc123&arg=45" to "http://www.someservice.com?service=wfs&layer=abc123xxx&arg=0.70710",
					then return someservice's response in xml format.

			#fit.R
				:markdown
					R engines are not yet implemented.

			#fit.ma
				:markdown
					# Stateless Example1
					
					This [mat-like engine](/engine.view?engine=mat&name=demo1):

						Code:
							Z = [1,2;3,4];
							X = A * A';
							Y = B * B';

						Context: {
							entry: {
								A: "SELECT a2,a3,a6 FROM app1.MATtest WHERE least(?,1)",
								B: "SELECT a1,a6 FROM app1.MATtest WHERE least(?,1)"
							}
						}

					illustrates how its context variables *A* and *B* are imported with the sql
					*entry* hash.  The sql ?-tokens in the *entry* hash evaluate with the 
					engine query, for example, [? = {Name:test}](/demo1.db?name=test).
					On [executing](/demo1.db?name=test) this engine places  
					variables *X*, *Y*, and *Z* into its context.  When an engine terminates,
					it it free to store any context variables into the sql store with its sql
					*exit* hash.

					#Stateless Example2

					[Executing](/demo2.db?name=test) [this](/engine.view?engine=mat&name=demo2) engine

						Code:
							Z = [1,2;3,4];
							X = A * A';
							Y = B * B';
							R = addIt(1,2);

						Context: {
							entry: {
								A: "SELECT a2,a3,a6 FROM app1.MATtest WHERE least(?,1)",
								B: "SELECT a1,a6 FROM app1.MATtest WHERE least(?,1)"
							},

							require: {
							   addIt :  function (a,b) { return a+b; }
							}
						}

					shows how Matlab-like engines can be readily extended with the **require** hash.

					#Stateful Example

					Stateful engines are operated in [workflows](/nodeflow.view) and extend 
					stateless engines with the **ports** hash.  These ports are defined by 
					functions that hold/latch its input/output events, thus maximizing data 
					stationarity in a workflow.  (More documentation required).

			#fit.sh
				:markdown
					The flexibility of Bash sh-engines comes with 
					considerable overhead and security implications; for these reasons, sh-engines
					are typically disabled. 

					These engines suffer o(1) second of overhead in loading/compiling a python/nodejs/etc module each time
					the engine is called.  This translates into a 6 hour overhead in a 	typical chipping workflow containing 
					20K chips/footprint.  When, however, workflows can be focused to a small area-of-interest, Bash 
					overhead can be tolerated.

					Bash engines are supported in both the HYDRA and **#{nick}** framework.  In
					the HYDRA framework, the engine's script is wrapped in a HYDRA proprietary 
					soapUI (nonrestful XML) interface serviced by HYDRA's web service.  In
					**#{nick}** framework, the engine's script is wrapped in a JSON (restful) interface
					serviced by **#{nick}**'s web service.  **#{nick}**'s service supports workflow engines (to 
					bypass the intrinsic overhead in calling sh-engines), as well as a mechanisms 
					to directly interface with clients and other workflow engines. And whereas 
					**#{nick}** is PKI driven, HYDRA is login driven.

					This [demo engine](/engines.db) example (test [here](/demo.db)):

						Code:
							ls
							python mypgm.py
						Context:
							{ key: value, ...}

					illustates an engine that simply list the files in the current directory,
					the call the mypgm python module.

			#fit.jade
				:markdown
					Jade engines contain [Jade markdown](/skinguide.view) to manage client
					views.  A Jade engine is invoked at a **view end-point** with optional
					parameters defined by the Jade engine.

			//
				#fit.Virtual
				:markdown
					Virtual-dataset engines are JS-engines that can be accessed and customized
					ny right-clicking its corresponding action button from within a view.  Virtual dataset
					engines follow this pattern:

						Code:
							function (req, res) {   		// request-callback
								var sql = req.sql, 			// sql connector hash
									query = req.query;		// request query parameters

								sql.query("...", 			// sql query with ?-tokens
									query, 					// query parameters
									function (err,data) {  	// query error and data
										res(err || data);	// return error or data
								});
							}
						Context:
							{ key: value, ...}

					The response callback **res** must be called, and must return either
					an **Error** object, a **String** message, an **Array** list of
					records, or an **Object** hash.

			#fit.sq
				:markdown
					This [sql engine](/engine.view?engine=sql&name=demo):

						Code:
							SQL.select = function (sql,recs,cb) {
								var q = sql.query("SELECT * FROM ?? WHERE ?",["intake",{TRL:2}])
								.on("result", function (rec) {
									rec.Cat = rec.Name + rec.Tech;
									recs.push(rec);
								})
								.on("end", function () {
									CON.log("returning recs="+recs.length);
									cb(recs);
								});
								CON.log("sql command="+q.sql);
							}
						Context:
							{ key: value, ...}

					is a CRUD-select [simply selects)(/demo.sql) all records from the **intake** 
					dataset whose **TRL** is at 2, and adds a **Cat** field to each record.  Note 
					again that all i/o (here console.log) is sent to the **service** console.  Note 
					too that when this engine is executed (read/GET) for the first time, the engine is 
					simply added to **#{nick}**; subsequent executions return the desired records to 
					the client.

			//
				#fit.Proxy
				:markdown
					Proxy engines are [engines](/engine.view) that run periodically given a
					run interval Period parameter.  A 0-period proxy is run once at **#{nick}** startup.
					The [job hawkers](/admin.view) are proxy engines.

			#fit.mo
				:markdown
					[Model engines](/engine.view) are used/defined by workflows when 
					systems are referenced/saved from within the [workflow editor](/nodeflow.view).  
					Model engines should remain disabled to prevent execution.

			#fit.cv
				:markdown
					cv-machines learn, locate and classify objects.  The [haar engine](/engine.view?engine=cv&name=haar),
					for example, executes a cv-machine in the following context:
					
						{ 
							frame: {
								job: jpg file to load and examine
							},
							detector: {
								scale: 0:1 ,
									//specifies how much the image size is reduced at each image scale step, and thus defines a 
									//scale pyramid during the detection process.  E.g. 0.05 means reduce size by 5% when going to next image 
									//scale step.  Smaller step sizes will thus increase the chance of detecting the features at diffrent scales. 
								delta: 0:1 ,
									//features of dim*(1-delta) : dim*(1+delta) pixels are detected
								dim: integer ,
									//defines nominal feature size in pixels
								hits: integer ,
									//specifies number is required neighboring detects to declare a single detect.  A higher value
									//results in less detections of higher quality. 3~6 is a good value.
								cascade: [ "path to xml file", ... ] ,
									//list of trained cascades
								net: string
									//path to prototxt file used to train caffe cnn	
							}
						}

			#fit.py
				:markdown
					## Stateless Engines

					In this [python engine](/engine.view?engine=py&name=demo1):

						Code:
							print "welcome to python"
							tau = [{'x':[11,12],'y':[21,22]}]
							print tau
							print query

							SQL0.execute("SELECT * from Htest", () )
							for (Rec) in SQL0:
								print Rec

						Context: 
							{ }

					we display *Htest* sql data at the service console, then return a *tau* JSON data 
					to the client.  Engine results (returned *tau*) can be: accessed from 
					a [#grid(path="/demo1.db",cols="x,y") skin](/skinguide.view), viewed
					directly [/demo1.view](/demo1.view), or simply dumped [/demo1.py](/demo1.py).
					As well as the 	standard import-export context support (not employed here), Python 
					engines are provided two sql cursors -- *SQL0* and *SQL1* for read and write -- 
					as [demonstrated here](/demo1.db).

					## Stateful Engines

					Stateful engines are used to maximize data stationarity in workflows.  This
					[demo2 engine](/engines.db?engine=py&name=demo2) example:

						Code:
							print "executing on port " + port

							def f1(tau,parm):
								print "in port f1"
								return 0

							def f2(tau,parm):
								print "in port f2"
								return 0

						Context: {
							f1: {a:1, b:2}, 
							f2: {a:2, b:1} 
						}

					defines input/output ports *f1* / *f2* to be called during the workflow 
					with the incoming *tau* event token and the assoicated *parm* hash taken 
					from the defined *ports* hash.  Although this engine would be typically used
					in [stateful flows](/nodeflow.view), it can also be used in a stateless flow
					by passing *port* (as demonstrated [here](/demo2.py?port=f1)).

			#fit.js
				:markdown
					## Stateless Engines

					Consider this [js engine](/engine.view?engine=js&name=demo1):

						Code:
							CON.log("M = "+M+" name="+query.Name);
							CON.log("A="+A.length+" by "+A[0].length);
							CON.log("B="+B.length+" by "+B[0].length);
							tau[0].u = [M,M+1,M+2];

						Context: {
							M: 3, 
							"case": "file1",
							entry: {
								A: "SELECT a2,a3,a6 FROM MATtest WHERE least(?,1)",
								B: "SELECT a1,a6 FROM MATtest WHERE least(?,1)"
							},
							exit: {
								A: "INSERT INTO ?? SET ?"
							}
						}

					In this example, the [data returned](/demo1.js?name=test) by *tau* can, 
					for example, be fed to a [dynamic view](/demo1.view).

					On entry to this engine, its context *A* and *B* variables are primed by the entry queries.  On
					exit, its context *B* variable is exported by the exit query.  Engines (both stateful  and stateless) are 
					always free to revise their context as needed.

					An engine's context is aways provided a *sql* connector
					equipped with the *sql.query* and *sql.release* methods to directly access a database
					(limited only by the client's profile group).  This *sql* connector also provides 
					*insertJob*, *deleteJob*, *updateJob* and *selectJob* methods to
					minipilate jobs in the FLEX queues.

					Because this engine did not define functions -- functions are associated
					with workflow engine ports -- this stateless engine is [only accessible at an endpoint](/demo1.view?name=test).

					## Stateless Engines

					This [js engine](/engine.view?engine=js&name=demo2):

						Code:
							var Name = query.Name;
							var scope = {X:X, y:y};
							var N = X.length;

							MATH.eval("a = inv(X' * X) * X' * y",scope);
							CON.log(scope.a);

							var b = {Name:Name,Tests:N,Computed:new Date()}, a=scope.a;
							for (var n=0,N=a.length;n<N;n++) b[n] = a[n];

							CON.log(b);

						Context: {
							M: 3,
							entry: {
								X: "SELECT p0,p1,p2 FROM Htest WHERE least(?,1)",
								y: "SELECT FPR from Htest WHERE least(?,1)"
							}
						}

					can be [tested here](/demo2.js?name=file1&Used=1), and makes use of the mathjs 
					MATH module to do a Matlab-like regression analysis.  Here, the **Name**, **Used** 
					and **M** parameters -- acquired from the URL query -- are used to retrieve data 
					from the **Htest** dataset.  This data is then used to setup a regression companion 
					matrix **X** and measurement vector **y**.  Regression results **a** are then saved into 
					a **b** vector (which, for example, may be saved with a Context.entry sql).

					Automatically added to the context are the following libraries:

						CON console [http://mathjs.org/docs]
						MATH Matlab-like scripting [http://mathjs.org/docs]
						DSP digital signals [https://www.npmjs.com/package/digitalsignals]
						CRYPTO cryptological [http://nodejs.org/api]
						LWIP light-weight imaging processing [https://github.com/EyalAr/lwip]
						SQL mysql connector [https://www.npmjs.com/package/mysql]
						JSON standard json parsing [w3schools.com]							
						LIE  Lie group symmetry learners 
						INV Curve invariant learners
						TXP Transport equation learners

					## Stateful Example

					This [demo3 engine](/engine.view) example:

						Code:
							x = 123;
							function fi(tau,parm) {
							  return 0;
							}; 

							function fo(tau,parm) {
							  return 0;
							};

						Context: { 
							fi : {x:10, y:20},
							fo: {x:11, y:21} 
						}

					places **x** and the ports **fi**, **fo** in its context, as [demonstrated here](/demo3.js).
					Subsequent requests at the step/POST endpoint with **port** = "fi" or "fo" will call the
					function with the current **tau** workflow events and the **parm** set to the 
					corresponding "ports" hash.

	#accordion.Machines
		:markdown
			An Engines employ the proper MAC = opencv | python | ... machine implemented under
			**engine/ifs/MAC/MAC.cpp**.  These machines are bound to #{title} using the 
			*node-gyp rebuild* provided by *maint.sh bind*.  
			
			All opencv-machines implement the following pattern:

				class OPORT { 								 	// output port
					OPORT(str Name, V8OBJECT Parm) { 		// Initiallize 
					};
					// Members follow
				};
				class IPORT { 								 	// input port
					IPORT(str Name, V8OBJECT Parm) { 		// Initiallize 
					};
					// Members follow						
				};
				class FEATURE { 							// Output object
					FEATURE( ... ) {						// Initialize 
					};	
					// Members follow
				};
				class CVMACHINE : public MACHINE {  
					int atch(IPORT &port, V8ARRAY tau) { 	// Latch input context to input port
						return 0; // if successful
					}
					int latch(V8ARRAY tau, OPORT &port) { 	// Latch output port to output context
						return 0; // if successful
					}
					int program (void) { 		// program and step machine
					}
					int call(const V8STACK& args) {  // nodejs interface
					}
					// Members follow
				}

			When bound to #{title} (using *node-gyp rebuild* provided by *maint.sh bind*), a **pool** of 
			(typically 256) MAC machines is reserved to run multiple (context independent) compute 
			threads at

					error = MAC.call( [ id string, code string, context hash ] )

			this call returning an interger error code (non-zero if a fault occured).

			The thread id (typically "Name.Client.Instance") uniquely identifies the compute thread.  
			Compute threads can be freely added to the pool until the pool becomes full.  

			When stepping a machine, the code string specifies either the name of the engine port on 
			which the arriving context is latched, or the name of the output port on which the departing 
			context is latched; thus stepping the machine in a stateful way (to maximize data restfulness).
			Given, however, an empty code string , the machine is stepped in a stateless way, that is, 
			by latching context to all input ports, then latching all output ports to the context.

	#accordion.System
		:markdown
			The following routes are used to maintain #{title}:

				GET /SITE.yql 	Return data from YQL compliant SITE=select/from/where
				GET /socket.io	reserved client socket.io connections
				GET /ping	check client-server connectivity
				GET /alert	broadcast alert &msg to all clients
				GET /stop	stops the server with alert &msg broadcasted to all clients
				GET /bit	built-in test with &N client connections at rate &lambda=events/s
				GET /service/algorithm/ENGINE Execute ENGINE with SOAP/XML parameters
				GET /user 	select,delete,update,insert user profile
				GET /wget	fetch data from wget endpoint using query parameters
				GET /curl	fetch data from curl endpoint using query parameters
				GET /http	fetch data from http endpoint using query parameters
				GET /test	challenge response endpoint

//- UNCLASSIFIED
