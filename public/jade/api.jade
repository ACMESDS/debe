// UNCLASSIFIED 

extends site
append site_help
	:markdown
		See also the companion [skinning guide](/skinguide.jade) and the [programmers ref manual](/shares/doc/**#{title}**/index.html).
append site_parms
	- view = "Tabbed"
	- dock = "top"
append site_body

	#fit.Intro

		:markdown
			#{title} provides a [cloud computing service](https://totem.west.ile.nga.ic.gov/api.view) for producing geoint products.
			#{title} uses [TOTEM endpoints](https://github.com/acmesds/totem):

				POST /NODE ?? NODE ...
				GET /NODE ?? NODE ...
				PUT /NODE ?? NODE ...
				DELETE /NODE ?? NODE ...

			to access its NODEs.  A NODE references a dataset, notebook, file or command:

				DATASET.TYPE ? QUERY
				NOTEBOOK.TYPE ? QUERY
				FILE.TYPE ? QUERY
				COMMAND.TYPE ? QUERY

			where (see [API](https://totem.west.ile.nga.ic.gov/api.view) and 
			[skinning guide](https://totem.west.ile.nga.ic.gov/skinguide.view)) TYPE 
			will either convert DATASET:

				db | xml | csv | txt | flat | kml | html | json

			inspect DATASET:

				tree | schema | nav | stat | delta

			render NOTEBOOK:

				view | run | pivot | site | spivot | brief | gridbrief | pivbrief | runbrief | proj

			probe NOTEBOOK:

				exe | tou | md | status | suitors | usage | EVENTS

			manage NOTEBOOK:

				import | export | publish | addkey | subkey

			license NOTEBOOK:

				js | py | m | me | jade | ...

			#{title} provides the following COMMANDs:

				agent | alert | ingest | riddle | task | ping

			for distributing jobs, alerting clients, inngesting data, validating sessions, sharding tasks, and 
			testing connections.  In addition, #{title} establishes FILE areas: 

				stores | uploads | shares

			for uploading, storing and serving files.

	#accordion.Views

		#fit.Endpoints

			:markdown
				Views are customizable [Jade skinning engines](/skinguide.view) that define a client's view, 
				and are accessed at the following routes:

					GET	/SKIN.view	Render SKIN from enabled engines or internal jade files
					GET	/DATASET.view	Dynamically generate a skin for this DATASET

		#fit.Queries
			:markdown
				Views parameters are SKIN dependent. For example, [plot.view?help](/plot.view?help) will 
				generate help for the *plot.view*.

		#fit.Examples
			:markdown
				[view plot help](/plot.view?help)  
				[view the news dataset](/news.view)  
				[view the jsdemo1 notebook](/jsdemo1.run)  
				[view the skinning guide](/skinguide.view)  
				[view the api](/api.view)  
				[view a sample application](/swag.view)  
				[view models](/flow.view)  
				[view a sample briefing](/home_brief.view)  
				[view briefing under the ELT1 area](/ELT1.home_brief.view)

	#accordion.Files
		#fit.Endpoints
			:markdown
				File access is provided at the following endpoints:

					GET	/AREA/FILE.TYPE?QUERY	Return FILE from AREA using query parameters
					GET	/ATTR/FILE.TYPE?QUERY	Return ATTRribute of a FILE
					GET	/AREA/	Return list of files in this AREA

				where AREA references a *#{title}* internal file store or a one of its virtual stores:

					code	file pulled from the [engines db](/engine.view)
					jade	file pulled from the [engines db](/engine.view) or (failing that) the jade file area
					uploads	file pulled from delete-on-access area
					stores	file pulled from long-term area
					positives	file pulled from positive-proof area
					negatives	file pulled from negative-proof area
					cert	generates a PKI cert for the requested user
					sim		reserved for simulation engines
					chips	image chipping cache
					tips	image tipping cache
					shares 	spot to place skinning content
					
		#fit.Queries
			:markdown
				File retrieval:
				> _has	find best file by string containment   
				> _nlp	find best file by nlp context  
				> _bin	find best file by binary expression  
				> _qex	find best file by query expansion  
				> _score	minimum score required

				File upload body JSON parameters:
				
					key = val ; key = val ; ...  [cr newline]  
					:  
					:  
					file data [cr newline]

		#folder.Readers
			#fit.Intro
				:markdown
					Readers are builtin [engines](/engine.view) that automatically index a variety of 
					document, graphics, presentation, and spreadsheet files when uploaded into
					*#{title}*.  Ingested text is checked for readibility, indexed to the best
					using [NLP training rules](/admins.view), then reflected into the [file stores](/files.view).

			#fit.Special
				code.
					html	- Web site
					rss	- News feed
					idop	- NTM imagery

			#fit.Document
				code.
					bib      - BibTeX [.bib]
					doc      - Microsoft Word 97/2000/XP [.doc]
					doc6     - Microsoft Word 6.0 [.doc]
					doc95    - Microsoft Word 95 [.doc]
					docbook  - DocBook [.xml]
					docx     - Microsoft Office Open XML [.docx]
					docx7    - Microsoft Office Open XML [.docx]
					fodt     - OpenDocument Text (Flat XML) [.fodt]
					html     - HTML Document (OpenOffice.org Writer) [.html]
					latex    - LaTeX 2e [.ltx]
					mediawiki - MediaWiki [.txt]
					odt      - ODF Text Document [.odt]
					ooxml    - Microsoft Office Open XML [.xml]
					ott      - Open Document Text [.ott]
					pdb      - AportisDoc (Palm) [.pdb]
					pdf      - Portable Document Format [.pdf]
					psw      - Pocket Word [.psw]
					rtf      - Rich Text Format [.rtf]
					sdw      - StarWriter 5.0 [.sdw]
					sdw4     - StarWriter 4.0 [.sdw]
					sdw3     - StarWriter 3.0 [.sdw]
					stw      - Open Office.org 1.0 Text Document Template [.stw]
					sxw      - Open Office.org 1.0 Text Document [.sxw]
					text     - Text Encoded [.txt]
					txt      - Text [.txt]
					uot      - Unified Office Format text [.uot]
					vor      - StarWriter 5.0 Template [.vor]
					vor4     - StarWriter 4.0 Template [.vor]
					vor3     - StarWriter 3.0 Template [.vor]
					xhtml    - XHTML Document [.html]

			#fit.Graphics
				code.
					bmp      - Windows Bitmap [.bmp]
					emf      - Enhanced Metafile [.emf]
					eps      - Encapsulated PostScript [.eps]
					fodg     - OpenDocument Drawing (Flat XML) [.fodg]
					gif      - Graphics Interchange Format [.gif]
					html     - HTML Document (OpenOffice.org Draw) [.html]
					jpg      - Joint Photographic Experts Group [.jpg]
					met      - OS/2 Metafile [.met]
					odd      - OpenDocument Drawing [.odd]
					otg      - OpenDocument Drawing Template [.otg]
					pbm      - Portable Bitmap [.pbm]
					pct      - Mac Pict [.pct]
					pdf      - Portable Document Format [.pdf]
					pgm      - Portable Graymap [.pgm]
					png      - Portable Network Graphic [.png]
					ppm      - Portable Pixelmap [.ppm]
					ras      - Sun Raster Image [.ras]
					std      - OpenOffice.org 1.0 Drawing Template [.std]
					svg      - Scalable Vector Graphics [.svg]
					svm      - StarView Metafile [.svm]
					swf      - Macromedia Flash (SWF) [.swf]
					sxd      - OpenOffice.org 1.0 Drawing [.sxd]
					sxd3     - StarDraw 3.0 [.sxd]
					sxd5     - StarDraw 5.0 [.sxd]
					sxw      - StarOffice XML (Draw) [.sxw]
					tiff     - Tagged Image File Format [.tiff]
					vor      - StarDraw 5.0 Template [.vor]
					vor3     - StarDraw 3.0 Template [.vor]
					wmf      - Windows Metafile [.wmf]
					xhtml    - XHTML [.xhtml]
					xpm      - X PixMap [.xpm]

			#fit.Presentation
				code.
					bmp      - Windows Bitmap [.bmp]
					emf      - Enhanced Metafile [.emf]
					eps      - Encapsulated PostScript [.eps]
					fodp     - OpenDocument Presentation (Flat XML) [.fodp]
					gif      - Graphics Interchange Format [.gif]
					html     - HTML Document (OpenOffice.org Impress) [.html]
					jpg      - Joint Photographic Experts Group [.jpg]
					met      - OS/2 Metafile [.met]
					odg      - ODF Drawing (Impress) [.odg]
					odp      - ODF Presentation [.odp]
					otp      - ODF Presentation Template [.otp]
					pbm      - Portable Bitmap [.pbm]
					pct      - Mac Pict [.pct]
					pdf      - Portable Document Format [.pdf]
					pgm      - Portable Graymap [.pgm]
					png      - Portable Network Graphic [.png]
					potm     - Microsoft PowerPoint 2007/2010 XML Template [.potm]
					pot      - Microsoft PowerPoint 97/2000/XP Template [.pot]
					ppm      - Portable Pixelmap [.ppm]
					pptx     - Microsoft PowerPoint 2007/2010 XML [.pptx]
					pps      - Microsoft PowerPoint 97/2000/XP (Autoplay) [.pps]
					ppt      - Microsoft PowerPoint 97/2000/XP [.ppt]
					pwp      - PlaceWare [.pwp]
					ras      - Sun Raster Image [.ras]
					sda      - StarDraw 5.0 (OpenOffice.org Impress) [.sda]
					sdd      - StarImpress 5.0 [.sdd]
					sdd3     - StarDraw 3.0 (OpenOffice.org Impress) [.sdd]
					sdd4     - StarImpress 4.0 [.sdd]
					sxd      - OpenOffice.org 1.0 Drawing (OpenOffice.org Impress) [.sxd]
					sti      - OpenOffice.org 1.0 Presentation Template [.sti]
					svg      - Scalable Vector Graphics [.svg]
					svm      - StarView Metafile [.svm]
					swf      - Macromedia Flash (SWF) [.swf]
					sxi      - OpenOffice.org 1.0 Presentation [.sxi]
					tiff     - Tagged Image File Format [.tiff]
					uop      - Unified Office Format presentation [.uop]
					vor      - StarImpress 5.0 Template [.vor]
					vor3     - StarDraw 3.0 Template (OpenOffice.org Impress) [.vor]
					vor4     - StarImpress 4.0 Template [.vor]
					vor5     - StarDraw 5.0 Template (OpenOffice.org Impress) [.vor]
					wmf      - Windows Metafile [.wmf]
					xhtml    - XHTML [.xml]
					xpm      - X PixMap [.xpm]

			#fit.Spreadsheet
				code.
					csv      - Text CSV [.csv]
					dbf      - dBASE [.dbf]
					dif      - Data Interchange Format [.dif]
					fods     - OpenDocument Spreadsheet (Flat XML) [.fods]
					html     - HTML Document (OpenOffice.org Calc) [.html]
					ods      - ODF Spreadsheet [.ods]
					ooxml    - Microsoft Excel 2003 XML [.xml]
					ots      - ODF Spreadsheet Template [.ots]
					pdf      - Portable Document Format [.pdf]
					pxl      - Pocket Excel [.pxl]
					sdc      - StarCalc 5.0 [.sdc]
					sdc4     - StarCalc 4.0 [.sdc]
					sdc3     - StarCalc 3.0 [.sdc]
					slk      - SYLK [.slk]
					stc      - OpenOffice.org 1.0 Spreadsheet Template [.stc]
					sxc      - OpenOffice.org 1.0 Spreadsheet [.sxc]
					uos      - Unified Office Format spreadsheet [.uos]
					vor3     - StarCalc 3.0 Template [.vor]
					vor4     - StarCalc 4.0 Template [.vor]
					vor      - StarCalc 5.0 Template [.vor]
					xhtml    - XHTML [.xhtml]
					xls      - Microsoft Excel 97/2000/XP [.xls]
					xls5     - Microsoft Excel 5.0 [.xls]
					xls95    - Microsoft Excel 95 [.xls]
					xlt      - Microsoft Excel 97/2000/XP Template [.xlt]
					xlt5     - Microsoft Excel 5.0 Template [.xlt]
					xlt95    - Microsoft Excel 95 Template [.xlt]
				
		#fit.Examples
			:markdown
				[download a file from the shares area](/shares/welcome.pdf)  
				[return flare json file from data area](/data/flare.json)

	#accordion.Datasets
		#fit.Endpoints
			:markdown
				Both real and virtual datasets are reached at the following endpoints:

					GET	/DATASET.TYPE		Return data from DATASET
					PUT	/DATASET.TYPE		Update DATASET with body parameters
					POST	/DATASET.TYPE	Insert body parameters into DATASET
					DELETE	/DATASET.TYPE	Delete from DATASET 

		#fit.Queries
			:markdown
				Relational:
				> KEY = VALUE || PATTERN
				> KEY <= VALUE  
				> KEY >= VALUE  
				> KEY != VALUE  
				> KEY < VALUE  
				> KEY > VALUE  

				Sampling:
				> ASKEY := KEY  
				> ASKEY := RELATIONAL  

				Keying json store:
				> STORE$[ INDEX ]  
				> STORE$.KEY  
				> STORE$[ INDEX ]..., .KEY...

				Grouping and Sorting:
				> _pivot = KEY,KEY,... pivot records on KEYs with NodeID = "ID,ID,..." groups  
				> _browse = KEY,KEY,... browse records on KEYs with NodeID = "name/name/ ..."  
				> _group = KEY,KEY,... group records on KEYs  
				> _sort	= KEY,KEY,... || [{property:KEY,sort:DIRECT}, ...] sort records on KEYs  
				> _trace = enable to trace sql at console  

				Misc:
				> _queue = KEY for queuing record state   
				> _nav = open | tree | rename | size ,root/A/B/... folder navigation  
				> _lock	= enable record locking  
				> _view = name of view to correlate with dataset  
				> _blog	= KEY blog markdown in record KEY

				Searching, Filtering, Limiting:
				> _nlp = pattern  
				> _exp = pattern  
				> _bin = pattern  
				> _limit = number of records to return  
				> _offset = record position to start returning records  
				> _score = minimum search score required  
				> _filters = [{property:KEY,value:PATTERN}, ...]

				Legacy:
				> _geo = KEY,KEY,... add geojson geometry KEYs to the records  
				> _json	= KEY,KEY,... json parse selected KEYs and index with KEY=IDX.IDX...  
				> _page	= page number (reserved)  
				> _mark	= KEY,KEY,... markdown these KEYs   
				> _jade	= KEY,KEY,... jadeify these KEYs (experimental - use _kjade)  
				> _kjade = KEY,KEY,... jadeify these KEYs (to be retired)

		#fit.Examples
			:markdown
				[return first 20 test records having x=123 and y is null sorted by u and b](/test.db?_start=0&_limit=10&x=123&y=null)   
				[return test records having x=123 and order by u and v](/test.db?sort=[{"property":"u","direction":"asc"},{"property":"v","direction":"asc"}]&x=123)   
				[return parms records](/parms.db)  
				[return parms records having specified parm](/parms.db?Parm=Band)  
				[return news records within certain age range](/news.db?age=690:693)  					
				[insert test record x=123,y=null](POST/test.db?x=123&y=null})  
				[update test record ID=10 with x=123,y=null ](PUT/test.db?x=123&y=null&ID=10)  
				[delete test record ID=10](DELETE/test.db?ID:10)  
				[return intake records](/intake.db)  
				[return intake records pivoted by TRL and Ver](/intake.tree?_group=TRL,VER)

		#fit.Virtual.Datasets
			#grid.DS.Parms(
				path="/parms.db",crush,
				head="Print,Help",
				cols="ID.a,Parm,Label,Type,Special,By Inspect.c,By Analysis.c,By Demo.c")

				:markdown
					The [parameter list](/parms.db) dataset defines how dataset fields are exposed to end 
					clients in grids, forms, folders, etc used in *#{title}* skins.  To each field corresponds a label name, verification 
					methods, and access priviledges.

			#grid.Roles(
				path="/roles.db",crush,
				head="Print,Help",
				cols="Table.T,Special.H,INSERT.T,UPDATE.T,DELETE.T,SELECT.T,IMPORT.T,EXPORT.T")

				:markdown
					The [user roles](/roles.db) dataset defines the roles assumed when clients insert, update, delete, and 
					select records from a specific dataset. 

			#grid.Intrinsic(
				path="/TABLES.db",crush,
				head="Print,Help",
				cols="Name.h")

				:markdown
					The [TABLES](/TABLES.db) provides a list of *#{title}* datasets.

			#grid.Admin(
				path="/ADMIN.db",crush,
				head="Print,Help",
				cols="TABLE_NAME,TABLE_TYPE,NOTEBOOK,VERSION,ROW_FORMAT,TABLE_ROWS,AVG_ROW_LENGTH,DATA_LENGTH,MAX_DATA_LENGTH,CREATE_TIME,UPDATE_TIME,TABLE_COMMENT")

				:markdown
					The [ADMIN](/ADMIN.db) provides detailed storage and technical information on all *#{title}* datasets.

			#grid.Sys.Config(
				path="/CONFIG.db",crush,
				head="Print,Help",
				cols="classif,extnet,disk,cpu,cpuat,platform,totalmem,freemem,uptime,cpus,host,netif,temp")

				:markdown
					System configuration information is available at [CONFIG](/CONFIG.db).

			:markdown
				The *#{title}* supports *Virtual Datasets* through its CRUDE (*create*, *read*, *update*, *delete*, 
				and *execute*) interface.  This CRUDE interface governs both *Database Datasets* and *Virtual Datasets*.
				For example, an *execute* on dataset X will typically import/export the data to/from dataset 
				X as controlled by its associated query parameters, a list of which might be returned by supplying a 
				&help parameter.  Below are several important **Virtual datasets**:

				+	[return upload files](/uploads.FILES.db), [stores files](/stores.FILES.db), etc
				+	[return jade views](/VIEWS.db)  
				+	[return connected users](/USERS.db)  
				+	[return engine summary](/ENGINES.db)  
				+	[return queue summary](/QUEUES.db)  
				+	[return work cliques](/CLIQUES.db)  
				+	[return system health](/HEALTH.db)  
				+	[return database activity](/ACTIVITY.db)  
				+	[return system configuration](/CONFIG.db)  
				+	[flatten searchable tables](/CATALOG.execute) for [search catalog](/CATALOG.db)  
				+	[contengency data](/ROCSUM.db)  
				+	[update](/events.execute) work predication [events](/events.db) and [stats](/jobstats.db)  
				+	[reflect git change logs](/issues.execute) to [tracked issues](/issues.db)  
				+	[broadcast messages](/sockets.execute) to [connected users](/sockets.db)  
				+	[import milestones](/milestone.execute) from internal spreadsheet
				
	#accordion.Notebooks
		#fit.Endpoints
			:markdown
				A notebook -- a dataset-engine pair -- is accessed from the following endpoints:

					GET /NOTEBOOK.exe?id=CASE 	Run NOTEBOOK in id-specified CASE context
					GET /NOTEBOOK.exe?name=CASE 	Run NOTEBOOK in name-specified CASE context
					GET /NOTEBOOK.exe?QUERY 	Run NOTEBOOK with QUERY context
					GET /NOTEBOOK.view	View and run NOTEBOOK contexts
					GET /NOTEBOOK.run		View, run, edit engine and manage NOTEBOOK

				A notebook's usecase context may include the following [optional keys](/api.view) ( add / remove ):  
				> [+](/#{ds}.add?Export=false) [/-](/#{ds}.sub?Export) *Export* switch writes notebook results into a file  
				> [+](/#{ds}.add?Ingest=false) [/-](/#{ds}.sub?Ingest) *Ingest* switch ingests notebook results into the database  
				> [+](/#{ds}.add?Share=false) [/-](/#{ds}.sub?Share) *Share* switch returns notebook results to the status area  
				> [+](/#{ds}.add?Pipe=doc) [/-](/#{ds}.sub?Pipe) *Pipe* json supervised chips, events, docs, images to the notebook  
				> [+](/#{ds}.add?Description=doc) [/-](/#{ds}.sub?Description) *Description* documents the usecase  
				> [+](/#{ds}.add?Config=doc) [/-](/#{ds}.sub?Config) *Config* js-script defines usecase context  
				> [+](/#{ds}.add?Entry=doc) [/-](/#{ds}.sub?Entry) *Entry* json primes context on entry using { KEY: "SELECT ....", ...}  
				> [+](/#{ds}.add?Exit=doc) [/-](/#{ds}.sub?Exit) *Exit* json saves context on exit using { KEY: "UPDATE ....", ...}  

				During execution, the Pipe will automatically add the keys:
				> *$* returned dataset (chips, events, docs, images, ....)
				> *Thread* name of compute thread

				and the notebook may itself add keys to retain usecase results:
				> *Save_STATE* aggregates events [ {at:"STATE", ...}, ... ]  
				> *Save_rem* collects remaining unaggregated events  
				> *Save_jpg* generates jpg from {at: "jpg": prime: "name", save: "name", index: [...], values: [...] } event  

				# Place a DATASET into a TYPE-specific supervised workflow using the Pipe:
				
					"/PATH/DATASET.TYPE?KEY=VALUE || $.JS ..."  
					{ "$": "MATHJS" }
					{ "Pipe": "/PATH/DATASET.TYPE?..." ,  "KEY": [VALUE, ...] , ... }

				The "/PATH/DATASET.TYPE" [source pipe](/api.view) defines the workflow based on TYPE = json || jpg | png | nitf || stream | export  || txt | doc | pdf | xls  || aoi || db,
				where $ references the TYPE-specific json data || GIMP image || event list || document text || db record.  The { KEY: [VALUE, ...] } [enumeration pipe](/api.view) 
				generates usecases over permuted context KEYs.  The $ json can be post-processed by a JS script or by a { $: "MATHJS" } [matlab-js scripting pipe](/api.view).
				
				## The { ... } enumeration pipe
				
				This enumeration pipe will generate and run all usecases defined by crossing the values of the specified context KEYs.  KEYs
				not in the notebook context are dropped.  The Pipe.Pipe KEY, typically a simply "path", can also be another enumeration pipe.  
				KEYs name "KEY.SUBKEY ..." can be used to set the SUBKEYs of a KEY json store.

				## The { ... } scripting pipe

				## The "/DATASET.json" pipe
				
				## The "/DATASET.jpg" pipe
				
				## The "/DATASET.stream" pipe

				## The "/DATASET.db" pipe
				
				## The "/DATASET.txt" pipe
				
				## The "/DATASET.aoi" pipe
				
				The aoi-pipe sources events ingested by the specified use-CASE of the specified DATASET, and accepts QUERY keys:
				
					group = "KEY, ..."  
					where = { KEY: VALUE, ...}  
					order = "KEY, ..."  
					limit = NUMBER  
					aoi = "NAME" || [ [lat,lon], ... ]  
					batch = NUMBER  // 0 disables  
					symbols = [ NUMBER, ... ]  
					keys = [ "KEY", ... ]  
					steps = NUMBER   // overrides file defaults  
					actors = NUMBER  // overrides file defaults
					
				to filter source events, and provides additional context keys:

					Host name of notebook  
					File context during a piped-workflow  
					Voxel context during a piped-workflow  
					Sensor context for current voxel  
					Chip  context with filepath for first jpeg collected in current voxel  
					Flux solar flux at earth's surface for current voxel  
					Events associated with current voxel  
					Flow context of workflow supervisor  
					Stats context shared with all notebooks

				where *Flow* contains the supervisors':   
				> *F* where F[k] = frequency of count k  
				> *T*  observation time [1/Hz]  
				> *J* where J[n] = number of jumps taken by n'th process at time T  
				> *N ensemble size  
				> *trP*	 where trP[n,m] = estimated state transition (from,to) probs at time T  
				> *store* event store at time T
						
				# *Description* may contain markdown:

					%{ PATH.TYPE ? w=WIDTH & h=HEIGHT & x=KEY$INDEX & y=KEY$INDEX ... }
					~{ TOPIC ? starts=DATE & ends=DATE ... }
					[ LINK ] ( URL )
					$$ inline TeX $$  ||  n$$ break TeX $$ || a$$ AsciiMath $$ || m$$ MathML $$  
					[JS || #JS || TeX] OP= [JS || #JS || TeX]  
					$ { KEY } || $ { JS } || $ {doc( JS , "INDEX" )}  
					KEY,X,Y >= SKIN,WIDTH,HEIGHT,OPTS  
					KEY <= VALUE || OP <= EXPR(lhs),EXPR(rhs)  

				block-escapes:
				
					HEADER:
						
						BLOCK
						
				and scripts:
				
					MARKDOWN
					script:
					MATLAB EMULATION SCRIPT
					
				to document notebook usecases.  For example:

					%{/plot.view?src=regress&name=test1&w=600&h=400&x=Save_train$.x[$chan]&y=Save_train$.y[$chan]&min=0,0&max=255,255}
					[go home](home.view?w=500&h=100)
					[go here grasshopper](https://here.gov/test.txt)
					%{/shares/a1.jpg}
					%{force.view?w=100&h=100&src=/queues?_pivots=class}
					$$ \alpha = 1 + \beta $$ impressive 'eh

				will embed: (1) a [d3 plot](/plot.view) of the x,y data from regress usecase "test1" using the chan widget, (2) 
				a link to the [home.view](/home.view), (3) a link to [the url](https://here.gov/test.txt), (4) the [image](/shares/a1.jpg), 
				(5) a [d3 force](/force.view) of the queues dataset pivoted by class, (6) an inline TeX equation, 
				See [plot](/plot.view?help), [treefan](/treefan.view?help), [treemap](/treefan.view?help), [force](/force.view?help), 
				[bounce](/bounce.view?help), [tipsheet](/tipsheet.view?help) for supported views. 				

		#fit.Publishing
			:markdown
				A NOTEBOOK  is published to *#{title}* using its publishing js-module (located at #{info.content})/TYPE/NOTEBOOK.js); these publishing
				modules are automaticallly run whenever *#{title}* starts, and when [user requested](/NOTEBOOK.pub).  NOTEBOOK publishing
				modules follow the following pattern:
				
					module.exports = {
						clear || reset : true || false,		// delete then recreate all usecases 
						
						mods || modkeys : {  		// change existing keys, e.g. MyFloatParm: "int(11)"
							KEY: "sql type", ...
						},
						
						adds || addkeys || keys : {  // add usecase keys of specified type, e.g. MyJsonParm: "json"
							KEY: "sql type", ...
						},
						
						inits || initial || initialize : // define initial usecases
							function () {
								return [ { KEY: VALUE, ...  }, ...];
							}
							
							||
						
							[ { KEY: VALUE, .... }, .... ],
						
						to : "py" || "js" || "m", // convert supplied engine code to another language
							// NOTE: smop poorly translates matlab [ [a,b,...]; [c,d, ...]; ...] matricies to python 
							// matricies.  Until fixed, manually convert "matlabarray(cat(...))" generated python  
							// to "matlabarray([ [a,b,...], [c,d,...], ....])".

						wrap: 					// js wrapper to coerce engine context ctx
							function (ctx, res, step) { 
								step(ctx, function (ctx) {
									res(ctx);
								});
							} 
							
							|| 
							
							( os ) => os.read || os.exec (`${os.path}.js`, ... ),	
 
						docs || dockeys : {  // key documentation
							key: doc, .....
						},
						
						tou || readme : 		
								// standard markdown with following tag extensions:
								//		<!---fetch url--->
								//  	<!---parms key=def&key=def&... ---> 
								// 		${key}
								
							" ... " 
							
							||
							
							( os ) => os.read || os.exec (`${os.path}.js`, ... ),
						
						subs || subkeys : { 		// keys and methods used to generate the ToU
							NAME: "...",
							totem: "...",
							by: "...",
							advrepo: "...",
							register: `<!---parms key=value--->`,
							input: (tags) => `<!---parms EVAL--->",
							fetch: (req, opts, input) => `<!---fetch EVAL--->input`,
							poc: "...",
							request: (req) => `[TO](EVAL)`,
							reqts: "...",
							summary: "...",
							ver: "...",
							now: "..."
						},
						
						state || context || ctx : {   // initial engine context
							key: value, ....
						},
						
						code || engine:  // the engine code
							function NAME(ctx,res) {   // js engine
								ctx.Save = [ ... ]; 
								res(ctx); 
							}
							
							||
							
							`def NAME(ctx,res):   # python engine
								ctx['Save'] = [ ... ];
								res(ctx) `
								
							||
							
							`function Save = NAME(ctx,res)		% matlab engine
								Save = [ ... ]; `
							
							||
							
							`Save = [ ... ]; `				// matlab-emulated engine
							
							||
							
							`... #grid.NAME(...) ...  `						// jade skinning engine
							
							|| 
							
							( os ) => os.read || os.exec (`${os.path}.js`, ... )
					}

				The client interprets KEYs of the form GROUP_SUBGROUP_NAME 
				as grouped (possibly r/o) keys; thus use "#" in place of "\_" if NAME contains an "\_".
				
		#fit.Examples
			:markdown
				See [published notebooks](/publist.view) for examples.
	
	#accordion.Agents
		#fit.Endpoints
			:markdown
				Engines can be outsourced to an agent at:

					GET /NOTEBOOK.exe?agent=AGENT&poll=N&QUERY
					GET /NOTEBOOK.exe?agent=AGENT&QUERY

				where the &poll request will start a job, then poll the AGENT every N seconds for its results.
				Conversely, a poll-less request will start a job, then reply on the AGENT to claim the job.  In
				either case, QUERY defines the NOTEBOOK args.

				**#{title}** reciprocates by providing its own AGENT at its **/agent** endpoint.

				Agents provide a means to outsource an engine, while retaining a thread
				on each agent request.  A valid agent must provide **#{title}** the
				following push/pull endpoints:

					GET http://AGENT?push=**#{title}**.CLIENT.NOTEBOOK.ID&args=JSON
					GET http://AGENT?pull=JOBID

				where the push endpoint defines the job being sent to the AGENT (with the CLIENT 
				requesting the agent, the NOTEBOOK being outsourced, the ID of the test case,
				and the JSON args to the NOTEBOOK).  The AGENT eventually responds at 
				the pull endpoint with a JOBID, or a "" if no job could be created.  **#{title}** will periodically 
				poll the agent for the results of JOBID.

		#fit.Examples
		
	#accordion.Engines

		#fit.Endpoints
			:markdown
				Simulation engines are available at these endpoints:

					GET /ENGINE.exe 	Compile and step ENGINE in a stateless workflow
					PUT /ENGINE.exe 	Compile ENGINE in a stateful workflow
					POST /ENGINE.exe	Step ENGINE  in a stateful workflow
					DELETE /ENGINE.exe	Free ENGINE from a stateful workflow

				Use the GET-endpoint to run stateless engines; use the PUT-, POST-, and DELETE-endpoints 
				to access stateful engines.  Whereas stateless engines (being memoryless) are initialized on a GET,
				stateful engines are: initialized when a [workflow](/nodeflow.view) issues a PUT, advanced when a
				workflow issues a POST, and reset when a workflow issues a DELETE, thus maximizing data 
				stationarity in a workflow.

				Please know that *#{title}* does not provide an interactive development environment; engines should be 
				thoroughly debugged in their native development environment before being [inserted](/engines.view) 
				into *#{title}*.  

		#fit.Queries
			:markdown
				An engine's inital context is held in its context JSON store:

					{
						"query": { "KEY": value, ... },
						"Entry": { "KEY": "select ...", ... } || "select ...",
						"Exit": { "KEY": "update ...", ... } || "update ...",
						"KEY": value, 
						"KEY": value, ...						
					}

				On entry, its context is primed using its *entry* sql-queries.  On exit, its context keys 
				can be exported by its *exit* sql-queries.   The "?" in sql-queries references the 
				context *query* (as overridden by url query parameters).  

		#folder.Examples(dock="left")
			#fit.R
				:markdown
					Although R engines are implemented, they have not yet been documented.

			#fit.sh
				:markdown
					The flexibility of Bash sh-engines comes with 
					considerable overhead and security implications; for these reasons, sh-engines
					are typically disabled. 

					These engines suffer o(1) second of overhead in loading/compiling a python/nodejs/etc module each time
					the engine is called.  This translates into a 6 hour overhead in a 	typical chipping workflow containing 
					20K chips/footprint.  When, however, workflows can be focused to a small area-of-interest, Bash 
					overhead can be tolerated.

					Bash engines are supported in both the HYDRA and *#{title}* framework.  In
					the HYDRA framework, the engine's script is wrapped in a HYDRA proprietary 
					soapUI (nonrestful XML) interface serviced by HYDRA's web service.  In
					*#{title}* framework, the engine's script is wrapped in a JSON (restful) interface
					serviced by *#{title}*'s web service.  *#{title}*'s service supports workflow engines (to 
					bypass the intrinsic overhead in calling sh-engines), as well as a mechanisms 
					to directly interface with clients and other workflow engines. And whereas 
					*#{title}* is PKI driven, HYDRA is login driven.

					This [sh-shell engine](/demo.db) example (test [here](/demo.db)):

						ls
						python mypgm.py
						
					with initial context: 
						
						{
							"KEY": value, ...
						}

					illustates an engine that simply list the files in the current directory,
					the call the mypgm python module.

			#fit.jade
				:markdown
					Jade engines contain [Jade markdown](/skinguide.view) to manage client
					views.  A Jade engine is invoked at a **view end-point** with optional
					parameters defined by the Jade engine.

			#fit.sq
				:markdown
					This [sql engine](/engine.view?engine=sql&name=demo):

						SQL.select = function (sql,recs,cb) {
							var q = sql.query("SELECT * FROM ?? WHERE ?",["intake",{TRL:2}])
							.on("result", function (rec) {
								rec.Cat = rec.Name + rec.Tech;
								recs.push(rec);
							})
							.on("end", function () {
								Log("returning recs="+recs.length);
								cb(recs);
							});
							Log("sql command="+q.sql);
						}

					with initial context:

							{ key: value, ...}

					is a CRUD-select [simply selects)(/demo.sql) all records from the **intake** 
					dataset whose **TRL** is at 2, and adds a **Cat** field to each record.  Note 
					again that all i/o (here console.log) is sent to the **service** console.  Note 
					too that when this engine is executed (read/GET) for the first time, the engine is 
					simply added to *#{title}*; subsequent executions return the desired records to 
					the client.

			#fit.m
				:markdown
					This [stateless matlab-engine](/mdemo1.run):

						function Save = mdemo1(ctx)
							Save = ctx.a + ctx.b;
						end
						
					returns the sum of its context *a* and *b* keys into its *Save* context key.

			#fit.me
				:markdown
					This [stateless emulated matlab engine](/medemo1.run):
					
						Save = a * (b + a);
						disp(Save);
						
					computes its *Save* context key given its *a* and *b* context keys.
					
					This [example](/engines.view):

						Z = [1,2;3,4];
						X = A * A';
						Y = B * B';

					with context:
					
						"Entry": {
							"A": "SELECT a2,a3,a6 FROM app.MATtest WHERE least(?,1)",
							"B": "SELECT a1,a6 FROM app.MATtest WHERE least(?,1)"
						}

					illustrates how its context variables *A* and *B* are imported with its sql-entry where
					the sql ?-tokens are sourced from the supplied query parameters.
					[For example](/demo1.db?name=test) places *X*, *Y*, and *Z* into its context after
					importing its *A* and *B* context keys.  When an engine terminates,
					it is free to store its context variables into its database with its sql-exit.

					This [emulated matlab example](/engines.view)

						Z = [1,2;3,4];
						X = A * A';
						Y = B * B';
						R = addIt(1,2);

					with context:
					
						"Entry": {
							"A": "SELECT a2,a3,a6 FROM app.MATtest WHERE least(?,1)",
							"B": "SELECT a1,a6 FROM app.MATtest WHERE least(?,1)"
						}

						"Require": {
						   "addIt" :  function (a,b) { return a+b; }
						}
						
					shows how these engines are extended with *Require*.

			#fit.mo
				:markdown
					[Model engines](/engine.view) are used/defined by workflows when 
					systems are referenced/saved from within the [workflow editor](/nodeflow.view).  
					Model engines should remain disabled to prevent execution.

			#fit.cv
				:markdown
					cv-machines learn, locate and classify objects.  The [haar engine](/engine.view?engine=cv&name=haar),
					for example, executes a cv-machine using a context:
					
						size = 50  	// feature size in [m]
						pixels = 512 	// samples across a chip [pixels]
						step = 0.01 	// relative seach step size
						range = 0.1 	// relative search size
						detects = 8		// hits required to declare a detect
						limit = 1e99 	// restrict maximum number of schips to ingest
						test = "test" 	// test case to store results
						scale = [0:1] || 8  		// scale^2 is max number of features in a chip

					which are related to (must cleanup this doc):
					
						{ 
							frame: {
								job: jpg file to load and examine
							},
							detector: {
								scale: 0:1 ,
									//specifies how much the image size is reduced at each image scale step, and thus defines a 
									//scale pyramid during the detection process.  E.g. 0.05 means reduce size by 5% when going to next image 
									//scale step.  Smaller step sizes will thus increase the chance of detecting the features at diffrent scales. 
								delta: 0:1 ,
									//features of dim*(1-delta) : dim*(1+delta) pixels are detected
								dim: integer ,
									//defines nominal feature size in pixels
								hits: integer ,
									//specifies number is required neighboring detects to declare a single detect.  A higher value
									//results in less detections of higher quality. 3~6 is a good value.
								cascade: [ "path to xml file", ... ] ,
									//list of trained cascades
								net: string
									//path to prototxt file used to train caffe cnn	
							}
						}

			#fit.py
				:markdown
					This [stateless python engine](/pydemo1.run):

						def pydemo1(ctx):
							print "welcome to python you lazy bird"

							SQL0.execute("SELECT * from app.Htest", () )
							for (rec) in SQL0:
								print rec

							ctx['Save'] = [ {'x':1, 'y':2, 'z':0}, {'x':3, 'y':4, 'z':10}]

					will log *Htest* data at the service console, then return *Save* to the notebook's context.  Whereas a
					notebook can store data in any CTX key, its *Save* key interfaces directly with the notebook
					workflow.  Python engines are also provided SYS, JSON, JIMP and CAFFE (if 
					on GPU VMs) libs, as well as the SQL0 (read) and SQL1 (write) cursors.
					
					This [stateful python engine](/pydemo2.exe) can be used in a workflow to 
					maximize data stationarity:

						def f1(tau,spec):
							print "in input port f1","events",tau,"port spec",spec
							return 0

						def f2(tau,spec):
							print "in output port f2","events",tau,"port spec",spec
							return 0

						print "executing on port " + PORT

					by defining its i/o ports *f1* / *f2* to be called during the workflow 
					with the incoming *tau* events at CTX['tau'], and the assoicated port *spec* at
					CTX['ports'][PORT] .  Although this engine would typically be used
					in [stateful flows](/nodeflow.view), it can also be used in a stateless flow
					by passing *port* (as demonstrated [here](/demo2.py?port=f1)).
					
			#fit.js
				:markdown
					# Stateful vs Stateless engines
					
					This [stateless js-engine](/jsdemo1.run):

						function jsdemo1(ctx, res) {
							Log("jsdemo1 ctx", ctx);
							var debug = false;

							if (debug) {
								Log("A="+ctx.A.length+" by "+ctx.A[0].length);
								Log("B="+ctx.B.length+" by "+ctx.B[0].length);
							}

							ctx.Save = [ {u: ctx.M}, {u:ctx.M+1}, {u:ctx.M+2} ];
							res(ctx);

							if (debug)
								$( "D=A*A'; E=D+D*3; disp(entry); ", ctx, (ctx) => {
									Log( "D=", ctx.D, "E=", ctx.E);
								});

					with initial context: 

						"M": 3, 
						"query": {  // default sql-entry query parms if none supplied on url
							"Name": "DefaultTestName"
						},
						"Entry": {
							"A": "SELECT a2,a3,a6 FROM MATtest WHERE least(?,1)",
							"B": "SELECT a1,a6 FROM MATtest WHERE least(?,1)"
						},
						"Exit": {
							"A": "INSERT INTO ?? SET ?"
						}

					will, on entry, prime its *A* and *B* context keys using its *entry* sql-queries: the "?" therein
					references its context *query* hash (as overridden by url query parameters).  On
					exit, its context *B* context variable is exported by its *exit* sql-query.   
					
					This [stateless js-engine](/jsdemo2.exe?name=test):

						function jsdemo2(ctx,res) {
							$("a = inv(X' * X) * X' * y", ctx, (ctx) => {
								Log(ctx);

								var 
									a = ctx.a,
									N = ctx.N = a.length,
									b = $(N, (n,B) => B[n] = a[n]);

								res(ctx);							
							});
						}

					with initial context: 
					
						"M": 3,
						"Entry": {
							"X": "SELECT p0,p1,p2 FROM Htest WHERE least(?,1)",
							"y": "SELECT FPR from Htest WHERE least(?,1)"
						}

					uses the mathjs MATH module to do regression analysis via an emulated Matlab machine.  Here, 
					the **Name**, **Used** and **M** parameters -- acquired from the URL and/or context query -- are 
					used to retrieve data 
					from the **Htest** dataset.  This data is then used to setup a regression companion 
					matrix **X** and measurement vector **y**.  Regression results **a** are then saved into 
					a **b** vector (which, for example, may be saved with a Context.entry sql).

					Whereas the previous engines are stateless -- they do not define functions (i.e. ports) -- this
					[stateful js-engine](/jsdemo3.exe):

						x = 123;
						function fi(tau,parm) {
						  return 0;
						}; 

						function fo(tau,parm) {
						  return 0;
						};

						function jsdemo3(ctx,res) {
						}

					with initial context: 
					
						{ 
							"fi" : {x:10, y:20},
							"fo": {x:11, y:21} 
						}

					defines input/output ports **fi** / **fo** as [demonstrated here](/demo3.js).
					
					Subsequent requests at the step/POST endpoint with **port** = "fi" or "fo" will call the
					function with the current **tau** workflow events and the **parm** set to the 
					corresponding "ports" hash.					

					# Library and notebook access
					
					js-notebooks may access the
					[matrix manipulator](https://sc.appdev.proj.coe.ic.gov://acmesds/man) $( "script", ctx => {} ), 
					[image chipper](https://sc.appdev.proj.coe.ic.gov://acmesds/geohack) $GEO, 
					[language parser](https://sc.appdev.proj.coe.ic.gov://acmesds/reader) $NLP( "doc", methods, metrics => {} ) ,
					[task sharding](https://sc.appdev.proj.coe.ic.gov://acmesds/totem) $TASK( { keys ... } , $ => {}, msg => {} ),  
					[db connector](https://sc.appdev.proj.coe.ic.gov://acmesds/totem) $SQL( sql => {} ),
					[image manipulator](https://sc.appdev.proj.coe.ic.gov://acmesds/man) $JIMP, 
					[neo4j cypher](https://sc.appdev.proj.coe.ic.gov://acmesds/debe), $NEO({query:"...", params:{...}}, (err,results) => {} ),   
					as well as [other notebooks](/tbd) $.notebook(ctx, ctx => {} ).
					
	#accordion.Machines
		:markdown
			All engines rely on a machine MAC = opencv | python | ... as implemented under
			**engine/ifs/MAC/MAC.cpp** and bound to **#{title}** using *node-gyp rebuild* as 
			provided by *maint.sh bind*.  
			
			opencv-machines, for example, implement the following pattern:

				class OPORT { 								 	// output port
					OPORT(str Name, V8OBJECT Parm) { 		// Initiallize 
					};
					// Members follow
				};
				class IPORT { 								 	// input port
					IPORT(str Name, V8OBJECT Parm) { 		// Initiallize 
					};
					// Members follow						
				};
				class FEATURE { 							// Output object
					FEATURE( ... ) {						// Initialize 
					};	
					// Members follow
				};
				class CVMACHINE : public MACHINE {  
					int atch(IPORT &port, V8ARRAY tau) { 	// Latch input context to input port
						return 0; // if successful
					}
					int latch(V8ARRAY tau, OPORT &port) { 	// Latch output port to output context
						return 0; // if successful
					}
					int program (void) { 		// program and step machine
					}
					int call(const V8STACK& args) {  // nodejs interface
					}
					// Members follow
				}

			When bound to **#{title}** (using *node-gyp rebuild* provided by *maint.sh bind*), a **pool** of 
			(typically 256) MAC machines is reserved to run multiple (context independent) compute 
			threads at

				error = MAC.call( [ id string, code string, context hash ] )
				error = MAC.call( [ id string, port string, context hash or event list] )

			this call returning an interger error code (non-zero if a fault occured).

			The thread id (typically "Name.Client.Instance") uniquely identifies the compute thread.  
			Compute threads can be freely added to the pool until the pool becomes full.  

			When stepping a machine, the code string specifies either the name of the engine port on 
			which the arriving context is latched, or the name of the output port on which the departing 
			context is latched; thus stepping the machine in a stateful way (to maximize data restfulness).
			Given, however, an empty code string , the machine is stepped in a stateless way, that is, 
			by latching context to all input ports, then latching all output ports to the context.

	#accordion.Commands
		:markdown
			The following commands maintain **#{title}**:

				GET /ping 		# check client-server connectivity
				GET /alert 		# broadcast alert &msg to all clients
				GET /riddle		# validate client when antibot protection configured
				GET /task 		# shard task to workers and nodes
				GET /help 		# request help from a poc
				GET /ingest		# start source ingest 
				GET /config 	# show system configuration
				GET /service/algorithm/NOTEBOOK # Execute NOTEBOOK with SOAP/XML parameters

//- UNCLASSIFIED
