// UNCLASSIFIED

extends site
append site_help
	:markdown
		See also the companion [skinning guide](/skinguide.jade) and the [programmers ref manual](/shares/doc/**#{title}**/index.html).
append site_parms
	- view = "Typical"
	- dock = "top"
append site_body

	#fit.Intro

		:markdown
			*#{title}* provides [CRUD-compliant endpoints](/api.view):

				POST		/NODE ?? NODE ...
				GET			/NODE ?? NODE ...
				PUT			/NODE ?? NODE ...
				DELETE 	/NODE ?? NODE ...

			to access dataset, plugin, file and command NODEs:

				DATASET.TYPE ? QUERY ? QUERY ...
				PLUGIN.TYPE ? QUERY ? QUERY ...
				FILEPATH.TYPE ? QUERY ? QUERY ...
				COMMAND.TYPE ? QUERY ? QUERY ...

			where the NODE TYPE can format returned data:

				db | xml | csv | txt | tab | tree | flat | kml | encap | html | json | geojson

			[render/skin](/skinguide.view) a plugin:

				view | pivot | site | spivot | brief | gridbrief | pivbrief | run | plugin | runbrief

			execute, extend/remove plugin keys:

				exe | add | sub

			return events ingested by a plugin:

				USECASE | js | py | ...

			retrieve a plugin attribute:

				tou | md | pub | status | suitors | js | py | m | me | jade | ...

			retrieve dataset attributes:

				delta | nav | stat

			or generate an office file:

				pdf | xdoc | xppt | xxls ...

			DEBE provides COMMAND endpoints:

				agent | alert | ingest | riddle | task | ping

			to interface with its job queues, users, data ingesters, session managers, task sharding, and
			test current session.

	#accordion.Views

		#fit.Endpoints

			:markdown
				Views are customizable [Jade skinning engines](/skinguide.view) that define a client's view, 
				and are accessed at the following routes:

					GET	/SKIN.view	Render SKIN from enabled engines or internal jade files
					GET	/DATASET.view	Dynamically generate a skin for this DATASET

		#fit.Queries
			:markdown
				Views parameters are SKIN dependent. For example, [plot.view?help](/plot.view?help) will 
				generate help for the *plot.view*.

		#fit.Examples
			:markdown
				[view plot help](/plot.view?help)  
				[view the news dataset](/news.view)  
				[view the jsdemo1 plugin](/jsdemo1.run)  
				[view the skinning guide](/skinguide.view)  
				[view the api](/api.view)  
				[view a sample application](/swag.view)  
				[view models](/flow.view)  
				[view a sample briefing](/home_brief.view)  
				[view briefing under the ELT1 area](/ELT1.home_brief.view)

	#accordion.Files
		#fit.Endpoints
			:markdown
				File access is provided at the following endpoints:

					GET	/AREA/FILE.TYPE?QUERY	Return FILE from AREA using query parameters
					GET	/ATTR/FILE.TYPE?QUERY	Return ATTRribute of a FILE
					GET	/AREA/	Return list of files in this AREA

				where AREA references a *#{title}* internal file store or a one of its virtual stores:

					code	file pulled from the [engines db](/engine.view)
					jade	file pulled from the [engines db](/engine.view) or (failing that) the jade file area
					uploads	file pulled from delete-on-access area
					stores	file pulled from long-term area
					positives	file pulled from positive-proof area
					negatives	file pulled from negative-proof area
					cert	generates a PKI cert for the requested user
					sim		reserved for simulation engines
					chips	image chipping cache
					tips	image tipping cache
					shares 	spot to place skinning content
					
		#fit.Queries
			:markdown
				File retrieval:
				> _has	find best file by string containment   
				> _nlp	find best file by nlp context  
				> _bin	find best file by binary expression  
				> _qex	find best file by query expansion  
				> _score	minimum score required

				File upload body JSON parameters:
				
					key = val ; key = val ; ...  [cr newline]  
					:  
					:  
					file data [cr newline]

		#folder.Readers
			#fit.Intro
				:markdown
					Readers are builtin [engines](/engine.view) that automatically index a variety of 
					document, graphics, presentation, and spreadsheet files when uploaded into
					*#{title}*.  Ingested text is checked for readibility, indexed to the best
					using [NLP training rules](/admins.view), then reflected into the [file stores](/files.view).

			#fit.Special
				code.
					html	- Web site
					rss	- News feed
					idop	- NTM imagery

			#fit.Document
				code.
					bib      - BibTeX [.bib]
					doc      - Microsoft Word 97/2000/XP [.doc]
					doc6     - Microsoft Word 6.0 [.doc]
					doc95    - Microsoft Word 95 [.doc]
					docbook  - DocBook [.xml]
					docx     - Microsoft Office Open XML [.docx]
					docx7    - Microsoft Office Open XML [.docx]
					fodt     - OpenDocument Text (Flat XML) [.fodt]
					html     - HTML Document (OpenOffice.org Writer) [.html]
					latex    - LaTeX 2e [.ltx]
					mediawiki - MediaWiki [.txt]
					odt      - ODF Text Document [.odt]
					ooxml    - Microsoft Office Open XML [.xml]
					ott      - Open Document Text [.ott]
					pdb      - AportisDoc (Palm) [.pdb]
					pdf      - Portable Document Format [.pdf]
					psw      - Pocket Word [.psw]
					rtf      - Rich Text Format [.rtf]
					sdw      - StarWriter 5.0 [.sdw]
					sdw4     - StarWriter 4.0 [.sdw]
					sdw3     - StarWriter 3.0 [.sdw]
					stw      - Open Office.org 1.0 Text Document Template [.stw]
					sxw      - Open Office.org 1.0 Text Document [.sxw]
					text     - Text Encoded [.txt]
					txt      - Text [.txt]
					uot      - Unified Office Format text [.uot]
					vor      - StarWriter 5.0 Template [.vor]
					vor4     - StarWriter 4.0 Template [.vor]
					vor3     - StarWriter 3.0 Template [.vor]
					xhtml    - XHTML Document [.html]

			#fit.Graphics
				code.
					bmp      - Windows Bitmap [.bmp]
					emf      - Enhanced Metafile [.emf]
					eps      - Encapsulated PostScript [.eps]
					fodg     - OpenDocument Drawing (Flat XML) [.fodg]
					gif      - Graphics Interchange Format [.gif]
					html     - HTML Document (OpenOffice.org Draw) [.html]
					jpg      - Joint Photographic Experts Group [.jpg]
					met      - OS/2 Metafile [.met]
					odd      - OpenDocument Drawing [.odd]
					otg      - OpenDocument Drawing Template [.otg]
					pbm      - Portable Bitmap [.pbm]
					pct      - Mac Pict [.pct]
					pdf      - Portable Document Format [.pdf]
					pgm      - Portable Graymap [.pgm]
					png      - Portable Network Graphic [.png]
					ppm      - Portable Pixelmap [.ppm]
					ras      - Sun Raster Image [.ras]
					std      - OpenOffice.org 1.0 Drawing Template [.std]
					svg      - Scalable Vector Graphics [.svg]
					svm      - StarView Metafile [.svm]
					swf      - Macromedia Flash (SWF) [.swf]
					sxd      - OpenOffice.org 1.0 Drawing [.sxd]
					sxd3     - StarDraw 3.0 [.sxd]
					sxd5     - StarDraw 5.0 [.sxd]
					sxw      - StarOffice XML (Draw) [.sxw]
					tiff     - Tagged Image File Format [.tiff]
					vor      - StarDraw 5.0 Template [.vor]
					vor3     - StarDraw 3.0 Template [.vor]
					wmf      - Windows Metafile [.wmf]
					xhtml    - XHTML [.xhtml]
					xpm      - X PixMap [.xpm]

			#fit.Presentation
				code.
					bmp      - Windows Bitmap [.bmp]
					emf      - Enhanced Metafile [.emf]
					eps      - Encapsulated PostScript [.eps]
					fodp     - OpenDocument Presentation (Flat XML) [.fodp]
					gif      - Graphics Interchange Format [.gif]
					html     - HTML Document (OpenOffice.org Impress) [.html]
					jpg      - Joint Photographic Experts Group [.jpg]
					met      - OS/2 Metafile [.met]
					odg      - ODF Drawing (Impress) [.odg]
					odp      - ODF Presentation [.odp]
					otp      - ODF Presentation Template [.otp]
					pbm      - Portable Bitmap [.pbm]
					pct      - Mac Pict [.pct]
					pdf      - Portable Document Format [.pdf]
					pgm      - Portable Graymap [.pgm]
					png      - Portable Network Graphic [.png]
					potm     - Microsoft PowerPoint 2007/2010 XML Template [.potm]
					pot      - Microsoft PowerPoint 97/2000/XP Template [.pot]
					ppm      - Portable Pixelmap [.ppm]
					pptx     - Microsoft PowerPoint 2007/2010 XML [.pptx]
					pps      - Microsoft PowerPoint 97/2000/XP (Autoplay) [.pps]
					ppt      - Microsoft PowerPoint 97/2000/XP [.ppt]
					pwp      - PlaceWare [.pwp]
					ras      - Sun Raster Image [.ras]
					sda      - StarDraw 5.0 (OpenOffice.org Impress) [.sda]
					sdd      - StarImpress 5.0 [.sdd]
					sdd3     - StarDraw 3.0 (OpenOffice.org Impress) [.sdd]
					sdd4     - StarImpress 4.0 [.sdd]
					sxd      - OpenOffice.org 1.0 Drawing (OpenOffice.org Impress) [.sxd]
					sti      - OpenOffice.org 1.0 Presentation Template [.sti]
					svg      - Scalable Vector Graphics [.svg]
					svm      - StarView Metafile [.svm]
					swf      - Macromedia Flash (SWF) [.swf]
					sxi      - OpenOffice.org 1.0 Presentation [.sxi]
					tiff     - Tagged Image File Format [.tiff]
					uop      - Unified Office Format presentation [.uop]
					vor      - StarImpress 5.0 Template [.vor]
					vor3     - StarDraw 3.0 Template (OpenOffice.org Impress) [.vor]
					vor4     - StarImpress 4.0 Template [.vor]
					vor5     - StarDraw 5.0 Template (OpenOffice.org Impress) [.vor]
					wmf      - Windows Metafile [.wmf]
					xhtml    - XHTML [.xml]
					xpm      - X PixMap [.xpm]

			#fit.Spreadsheet
				code.
					csv      - Text CSV [.csv]
					dbf      - dBASE [.dbf]
					dif      - Data Interchange Format [.dif]
					fods     - OpenDocument Spreadsheet (Flat XML) [.fods]
					html     - HTML Document (OpenOffice.org Calc) [.html]
					ods      - ODF Spreadsheet [.ods]
					ooxml    - Microsoft Excel 2003 XML [.xml]
					ots      - ODF Spreadsheet Template [.ots]
					pdf      - Portable Document Format [.pdf]
					pxl      - Pocket Excel [.pxl]
					sdc      - StarCalc 5.0 [.sdc]
					sdc4     - StarCalc 4.0 [.sdc]
					sdc3     - StarCalc 3.0 [.sdc]
					slk      - SYLK [.slk]
					stc      - OpenOffice.org 1.0 Spreadsheet Template [.stc]
					sxc      - OpenOffice.org 1.0 Spreadsheet [.sxc]
					uos      - Unified Office Format spreadsheet [.uos]
					vor3     - StarCalc 3.0 Template [.vor]
					vor4     - StarCalc 4.0 Template [.vor]
					vor      - StarCalc 5.0 Template [.vor]
					xhtml    - XHTML [.xhtml]
					xls      - Microsoft Excel 97/2000/XP [.xls]
					xls5     - Microsoft Excel 5.0 [.xls]
					xls95    - Microsoft Excel 95 [.xls]
					xlt      - Microsoft Excel 97/2000/XP Template [.xlt]
					xlt5     - Microsoft Excel 5.0 Template [.xlt]
					xlt95    - Microsoft Excel 95 Template [.xlt]
				
		#fit.Examples
			:markdown
				[download a file from the shares area](/shares/welcome.pdf)  
				[return flare json file from data area](/data/flare.json)

	#accordion.Datasets
		#fit.Endpoints
			:markdown
				Both real and virtual datasets are reached at the following endpoints:

					GET	/DATASET.TYPE		Return data from DATASET
					PUT	/DATASET.TYPE		Update DATASET with body parameters
					POST	/DATASET.TYPE	Insert body parameters into DATASET
					DELETE	/DATASET.TYPE	Delete from DATASET 

		#fit.Queries
			:markdown
				Relational:
				> KEY = VALUE  
				> KEY <= VALUE  
				> KEY >= VALUE  
				> KEY != VALUE  
				> KEY < VALUE  
				> KEY > VALUE  

				Indexing:
				> ASKEY := KEY  
				> ASKEY := RELATIONAL  

				Searching:
				> KEY, ... /= nlp pattern  
				> KEY, ... |= query expansion pattern  
				> KEY, ... ^= binary expression pattern  

				Keying json store:
				> STORE$[ INDEX ]  
				> STORE$.KEY

				Grouping and Sorting:
				> _pivot = KEY,KEY,... pivot records on KEYs with NodeID = "ID,ID,..." groups  
				> _browse = KEY,KEY,... browse records on KEYs with NodeID = "name/name/ ..."  
				> _group = KEY,KEY,... group records on KEYs  
				> _index = KEY,KEY,... return only specified KEYs  
				> _sort	= KEY,KEY,... sort records ascending on KEYs  
				> _sort	= [{property:KEY,sort:DIRECT}, ...] sort records asc|desc on KEYs   

				Converting:
				> _queue = KEY for queuing record state   
				> _nav = open | tree | rename | size ,root/A/B/... folder navigation  

				Blogging:  
				> _blog	= KEY blog markdown in record KEY

				Limiting:
				> _lock	= enable record locking  
				> _view = name of view to correlate with dataset  
				> _limit = number of records to return  
				> _start = record position to start returning records  
				> _score = minimum search score required

				Legacy:
				> _geo = KEY,KEY,... add geojson geometry KEYs to the records  
				> _json	= KEY,KEY,... json parse selected KEYs and index with KEY=IDX.IDX...  
				> _page	= page number (reserved)  
				> _mark	= KEY,KEY,... markdown these KEYs   
				> _jade	= KEY,KEY,... jadeify these KEYs (experimental - use _kjade)  
				> _kjade = KEY,KEY,... jadeify these KEYs (to be retired)

		#fit.Examples
			:markdown
				[return first 20 test records having x=123 and y is null sorted by u and b](/test.db?_start=0&_limit=10&x=123&y=null)   
				[return test records having x=123 and order by u and v](/test.db?sort=[{"property":"u","direction":"asc"},{"property":"v","direction":"asc"}]&x=123)   
				[return parms records](/parms.db)  
				[return parms records having specified parm](/parms.db?Parm=Band)  
				[return news records within certain age range](/news.db?age=690:693)  					
				[insert test record x=123,y=null](POST/test.db?x=123&y=null})  
				[update test record ID=10 with x=123,y=null ](PUT/test.db?x=123&y=null&ID=10)  
				[delete test record ID=10](DELETE/test.db?ID:10)  
				[return intake records](/intake.db)  
				[return intake records pivoted by TRL and Ver](/intake.tree?_group=TRL,VER)

		#fit.Virtual.Datasets
			#grid.DS.Parms(
				path="/parms.db",crush,
				head="Print,Help",
				cols="ID.a,Parm,Label,Type,Special,By Inspect.c,By Analysis.c,By Demo.c")

				:markdown
					The [parameter list](/parms.db) dataset defines how dataset fields are exposed to end 
					clients in grids, forms, folders, etc used in *#{title}* skins.  To each field corresponds a label name, verification 
					methods, and access priviledges.

			#grid.Roles(
				path="/roles.db",crush,
				head="Print,Help",
				cols="Table.T,Special.H,INSERT.T,UPDATE.T,DELETE.T,SELECT.T,IMPORT.T,EXPORT.T")

				:markdown
					The [user roles](/roles.db) dataset defines the roles assumed when clients insert, update, delete, and 
					select records from a specific dataset. 

			#grid.Intrinsic(
				path="/TABLES.db",crush,
				head="Print,Help",
				cols="Name.h")

				:markdown
					The [TABLES](/TABLES.db) provides a list of *#{title}* datasets.

			#grid.Admin(
				path="/ADMIN.db",crush,
				head="Print,Help",
				cols="TABLE_NAME,TABLE_TYPE,PLUGIN,VERSION,ROW_FORMAT,TABLE_ROWS,AVG_ROW_LENGTH,DATA_LENGTH,MAX_DATA_LENGTH,CREATE_TIME,UPDATE_TIME,TABLE_COMMENT")

				:markdown
					The [ADMIN](/ADMIN.db) provides detailed storage and technical information on all *#{title}* datasets.

			#grid.Sys.Config(
				path="/CONFIG.db",crush,
				head="Print,Help",
				cols="classif,extnet,disk,cpu,cpuat,platform,totalmem,freemem,uptime,cpus,host,netif,temp")

				:markdown
					System configuration information is available at [CONFIG](/CONFIG.db).

			:markdown
				The *#{title}* supports *Virtual Datasets* through its CRUDE (*create*, *read*, *update*, *delete*, 
				and *execute*) interface.  This CRUDE interface governs both *Database Datasets* and *Virtual Datasets*.
				For example, an *execute* on dataset X will typically import/export the data to/from dataset 
				X as controlled by its associated query parameters, a list of which might be returned by supplying a 
				&help parameter.  Below are several important **Virtual datasets**:

				+	[return upload files](/uploads.FILES.db), [stores files](/stores.FILES.db), etc
				+	[return jade views](/VIEWS.db)  
				+	[return connected users](/USERS.db)  
				+	[return engine summary](/ENGINES.db)  
				+	[return queue summary](/QUEUES.db)  
				+	[return work cliques](/CLIQUES.db)  
				+	[return system health](/HEALTH.db)  
				+	[return database activity](/ACTIVITY.db)  
				+	[return system configuration](/CONFIG.db)  
				+	[flatten searchable tables](/CATALOG.execute) for [search catalog](/CATALOG.db)  
				+	[contengency data](/ROCSUM.db)  
				+	[update](/events.execute) work predication [events](/events.db) and [stats](/jobstats.db)  
				+	[reflect git change logs](/issues.execute) to [tracked issues](/issues.db)  
				+	[broadcast messages](/sockets.execute) to [connected users](/sockets.db)  
				+	[import milestones](/milestone.execute) from internal spreadsheet
				
	#accordion.Plugins
		#fit.Endpoints
			:markdown
				A plugin -- a dataset-engine pair -- is accessed from the following endpoints:

					GET /PLUGIN.exe?id=CASE 	Run PLUGIN in id-specified CASE context
					GET /PLUGIN.exe?name=CASE 	Run PLUGIN in name-specified CASE context
					GET /PLUGIN.exe?QUERY 	Run PLUGIN with QUERY context
					GET /PLUGIN.view	View and run PLUGIN contexts
					GET /PLUGIN.run		View, run, edit engine and manage PLUGIN

				Its usecase context can include the following optional keys ( add to / remove from context):  
				> [+](/#{ds}.add?Export=false) [/-](/#{ds}.sub?Export) *Export* switch writes engine results into a file  
				> [+](/#{ds}.add?Ingest=false) [/-](/#{ds}.sub?Ingest) *Ingest* switch ingests engine results into the database  
				> [+](/#{ds}.add?Share=false) [/-](/#{ds}.sub?Share) *Share* switch returns engine results to the status area  
				> [+](/#{ds}.add?Pipe=doc) [/-](/#{ds}.sub?Pipe) *Pipe* json regulates chips and events to the engine  
				> [+](/#{ds}.add?Description=doc) [/-](/#{ds}.sub?Description) *Description* documents a usecase  
				> [+](/#{ds}.add?Config=doc) [/-](/#{ds}.sub?Config) *Config* js-script defines usecase context  
				> [+](/#{ds}.add?Save=doc) [/-](/#{ds}.sub?Save) *Save* aggregates results [{at:"STATE", ...}, ...] not captured in *Save_STATE* keys  
				> [+](/#{ds}.add?Entry=doc) [/-](/#{ds}.sub?Entry) *Entry* json primes context on entry using { KEY: "SELECT ....", ...}  
				> [+](/#{ds}.add?Exit=doc) [/-](/#{ds}.sub?Exit) *Exit* json saves context on exit using { KEY: "UPDATE ....", ...}  
				> [+](/#{ds}.add?Autorun=doc) [/-](/#{ds}.sub?Autorun) *Autorun* switch enables the autorun watchdog

				Use the *Pipe* = "/DATASET?QUERY" || "PLUGIN.CASE?QUERY" to bring || stream
				data into your plugin. When streamed, your plugin context will also contain
				the following supervisor keys:

					Host name of plugin  
					File context during a piped-workflow  
					Voxel context during a piped-workflow  
					Sensor context for current voxel  
					Chip  context with filepath for first jpeg collected in current voxel  
					Flux solar flux at earth's surface for current voxel  
					Events associated with current voxel  
					Flow context of workflow supervisor  
					Stats context shared with all plugins

				*Flow* contains the supervisors':   
				> *F* where F[k] = frequency of count k  
				> *T*  observation time [1/Hz]  
				> *J* where J[n] = number of jumps taken by n'th process at time T  
				> *N ensemble size  
				> *trP*	 where trP[n,m] = estimated state transition (from,to) probs at time T  
				> *store* event store at time T
						
				Valid *Description* markdown includes:

					[ post ] ( SKIN.view ? w=WIDTH & h=HEIGHT & x=BASE$X & y=BASE$Y & OPTS ) || BASE,X,Y >= SKIN,WIDTH,HEIGHT,OPTS  
					[ image ] ( PATH.jpg ? w=WIDTH & h=HEIGHT )  
					[ LINK ]( URL )  ||  [ FONT ]( TEXT )  ||  [ ]( URL )  ||  [TOPIC]( )  
					$$ inline TeX $$  ||  n$$ break TeX $$ || a$$ AsciiMath $$ || m$$ MathML $$ || [#EXPR || TeX] OP= [#EXPR || TeX]  
					\${ KEY } || \${ EXPR } || \${doc( EXPR , "IDX, ..." )}  
					KEY <= VALUE || OP <= EXPR(lhs),EXPR(rhs)  

				as well as block-escaping:
				
					HEADER:
						
						BLOCK
						
				and markdown scripting:
				
					MARKDOWN
					script:
					MATLAB EMULATION SCRIPT
					
				to document plugin usecases.  For example:

					[post](plot.view?w=100&h=200&x=Save$a&ys=Save$b$c)
					[post](home.view?w=500&h=100)
					[go here grasshopper](here/there.txt)
					[image](shares/a1.jpg)
					[force](force.view?w=100&h=100&src=/queues?_pivots=class)
					$$ \alpha = 1 + \beta $$ impressive 'eh
					[](http://localhost:8080)
					[jsdemo1]()

				will embed: (1) a plot of the requested x,y data using the [d3 plot](/plot.view), (2) the [home.view](/home.view), 
				(3) a link to the url, (4) the [image](/shares.jpg), (5) a force graph of the queues dataset pivoted by class using 
				the [d3 force](/force.view) into a frame of specified W x H dimensions, (6) an inline TeX equation, (7) contents
				of the localhost site, and (8) a smart tag, licensing the html content to the viewer as retained in the releases table.
				See [plot](/plot.view?help), [treefan](/treefan.view?help), [treemap](/treefan.view?help), [force](/force.view?help), 
				[bounce](/bounce.view?help), [tipsheet](/tipsheet.view?help) for example VIEWs. 				

		#fit.Publishing
			:markdown
				A PLUGIN  is published to *#{title}* using its publishing js-module (located at #{info.content})/TYPE/PLUGIN.js); these publishing
				modules are automaticallly run whenever *#{title}* starts, and when [user requested](/PLUGIN.pub).  PLUGIN publishing
				modules follow the following pattern:
				
					module.exports = {
						clear || reset : true || false,		// delete then recreate all usecases 
						
						mods || modkeys : {  		// change existing keys, e.g. MyFloatParm: "int(11)"
							KEY: "sql type", ...
						},
						
						adds || addkeys || keys : {  // add usecase keys of specified type, e.g. MyJsonParm: "json"
							KEY: "sql type", ...
						},
						
						inits || initial || initialize : // define initial usecases
							function () {
								return [ { KEY: VALUE, ...  }, ...];
							}
							
							||
						
							[ { KEY: VALUE, .... }, .... ],
						
						to : "py" || "js" || "m", // convert supplied engine code to another language
							// NOTE: smop poorly translates matlab [ [a,b,...]; [c,d, ...]; ...] matricies to python 
							// matricies.  Until fixed, manually convert "matlabarray(cat(...))" generated python  
							// to "matlabarray([ [a,b,...], [c,d,...], ....])".

						wrap: 					// js wrapper to coerce engine context ctx
							function (ctx, res, step) { 
								step(ctx, function (ctx) {
									res(ctx);
								});
							} 
							
							|| 
							
							( os ) => os.read || os.exec (`${os.path}.js`, ... ),	
 
						docs || dockeys : {  // key documentation
							key: doc, .....
						},
						
						tou || readme : 		
								// standard markdown with following tag extensions:
								//		<!---fetch url--->
								//  	<!---parms key=def&key=def&... ---> 
								// 		${key}
								
							" ... " 
							
							||
							
							( os ) => os.read || os.exec (`${os.path}.js`, ... ),
						
						subs || subkeys : { 		// keys and methods used to generate the ToU
							NAME: "...",
							totem: "...",
							by: "...",
							advrepo: "...",
							register: `<!---parms key=value--->`,
							input: (tags) => `<!---parms EVAL--->",
							fetch: (req, opts, input) => `<!---fetch EVAL--->input`,
							poc: "...",
							request: (req) => `[TO](EVAL)`,
							reqts: "...",
							summary: "...",
							ver: "...",
							now: "..."
						},
						
						state || context || ctx : {   // initial engine context
							key: value, ....
						},
						
						code || engine:  // the engine code
							function NAME(ctx,res) {   // js engine
								ctx.Save = [ ... ]; 
								res(ctx); 
							}
							
							||
							
							`def NAME(ctx,res):   # python engine
								ctx['Save'] = [ ... ];
								res(ctx) `
								
							||
							
							`function Save = NAME(ctx,res)		% matlab engine
								Save = [ ... ]; `
							
							||
							
							`Save = [ ... ]; `				// matlab-emulated engine
							
							||
							
							`... #grid.NAME(...) ...  `						// jade skinning engine
							
							|| 
							
							( os ) => os.read || os.exec (`${os.path}.js`, ... )
					}

				[cints](/js/cints.js), [rats](/js/rats.js), [trigs](/js/trigs.js), [genpr](/js/genpr.js), and [seppfm](/py/seppfm.js) provide
				example publishing modules for the [cints](/cints.run), [rats](/rats.run), [trigs](/trigs.run), [genpr](/genpr.run), 
				and [seppfm](/seppfm.run) plugins, their corresponding Terms-Of-Use [cints](/cints.tou), [rats](/rats.tou), 
				[trigs](/trigs.tou), [genpr](/genpr.tou), and [seppfm](/seppfm.tou); and their corresponding publishing endpoints
				[cints](/cints.pub), [rats](/rats.pub), [trigs](/trigs.pub), [genpr](/genpr.pub), and [seppfm](/seppfm.pub).
				
		#fit.Examples
			:markdown
				js ex1 [ui](/jsdemo1.run) [run usecase test1](/jsdemo1.exe?Name=test1) [get usecases](/jsdemo1)  
				js ex2 [ui](/jsdemo2.run) [run usecase test1](/jsdemo2.exe?Name=test1) [get usecases](/jsdemo2)  
				py ex1 [ui](/pydemo1.run) [run usecase test1](/pydemo1.exe?Name=test1) [get usecases](/pydemo1)  
				py ex2 [ui](/pydemo2.run) [run usecase test1](/pydemo2.exe?Name=test1) [get usecases](/pydemo2)  
				m ex1 [ui](/mdemo1.run) [run usecase test1](/mdemo1.exe?Name=test1) [get usecases](/mdemo1)  
				m ex2 [ui](/mdemo2.run) [run usecase test1](/mdemo2.exe?Name=test1) [get usecases](/mdemo2)  
	
	#accordion.Agents
		#fit.Endpoints
			:markdown
				Engines can be outsourced to an agent at:

					GET /PLUGIN.exe?agent=AGENT&poll=N&QUERY
					GET /PLUGIN.exe?agent=AGENT&QUERY

				where the &poll request will start a job, then poll the AGENT every N seconds for its results.
				Conversely, a poll-less request will start a job, then reply on the AGENT to claim the job.  In
				either case, QUERY defines the PLUGIN args.

				**#{title}** reciprocates by providing its own AGENT at its **/agent** endpoint.

				Agents provide a means to outsource an engine, while retaining a thread
				on each agent request.  A valid agent must provide **#{title}** the
				following push/pull endpoints:

					GET http://AGENT?push=**#{title}**.CLIENT.PLUGIN.ID&args=JSON
					GET http://AGENT?pull=JOBID

				where the push endpoint defines the job being sent to the AGENT (with the CLIENT 
				requesting the agent, the PLUGIN being outsourced, the ID of the test case,
				and the JSON args to the PLUGIN).  The AGENT eventually responds at 
				the pull endpoint with a JOBID, or a "" if no job could be created.  **#{title}** will periodically 
				poll the agent for the results of JOBID.

		#fit.Examples
		
	#accordion.Engines

		#fit.Endpoints
			:markdown
				Simulation engines are available at these endpoints:

					GET /PLUGIN.exe 	Compile and step PLUGIN in a stateless workflow
					PUT /PLUGIN.exe 	Compile PLUGIN in a stateful workflow
					POST /PLUGIN.exe	Step PLUGIN  in a stateful workflow
					DELETE /PLUGIN.exe	Free PLUGIN from a stateful workflow

				Use the GET-endpoint to run stateless engines; use the PUT-, POST-, and DELETE-endpoints 
				to access stateful engines.  Whereas stateless engines (being memoryless) are initialized on a GET,
				stateful engines are: initialized when a [workflow](/nodeflow.view) issues a PUT, advanced when a
				workflow issues a POST, and reset when a workflow issues a DELETE, thus maximizing data 
				stationarity in a workflow.

				Please know that *#{title}* does not provide an interactive development environment; engines should be 
				thoroughly debugged in their native development environment before being [inserted](/engines.view) 
				into *#{title}*.  

		#fit.Queries
			:markdown
				An engine's inital context is held in its context JSON store:

					{
						"query": { "KEY": value, ... },
						"Entry": { "KEY": "select ...", ... } || "select ...",
						"Exit": { "KEY": "update ...", ... } || "update ...",
						"KEY": value, 
						"KEY": value, ...						
					}

				On entry, its context is primed using its *entry* sql-queries.  On exit, its context keys 
				can be exported by its *exit* sql-queries.   The "?" in sql-queries references the 
				context *query* (as overridden by url query parameters).  

		#folder.Examples(dock="left")
			//
				#fit.url
				:markdown
					URL engines (experimental) provide a simple method to re-route a request
					based on URL parameters, ${req.PARM} and ${plugin.FN(req)} tags; such engines
					follow the pattern:

						"http://URL?PARM=${req.PARM}&PARM=${plugin.FN(req)}"
						
					with initial context:

						{ fn1: function (req) {...}, fn2: ... }

					For example, a **testeng** engine:

						"http://www.someservice.com/ogc?service=wfs&layer=${req.id+"xxx"}&arg=${plugin.angproj(req)}"
						
					with initial context:
					
						{ angproj: function (req) { 
							return Math.cos(req.arg*Math.PI/180); 
							} }

					would reroute "/testeng.xml?id=abc123&arg=45" to "http://www.someservice.com?service=wfs&layer=abc123xxx&arg=0.70710",
					then return someservice's response in xml format.

			#fit.R
				:markdown
					R engines are not yet implemented.

			//
				#fit.Proxy
				:markdown
					Proxy engines are [engines](/engine.view) that run periodically given a
					run interval Period parameter.  A 0-period proxy is run once at *#{title}* startup.
					The [job hawkers](/admin.view) are proxy engines.
			
				#fit.Virtual
				:markdown
					Virtual-dataset engines are JS-engines that can be accessed and customized
					ny right-clicking its corresponding action button from within a view.  Virtual dataset
					engines follow this pattern:

						function (req, res) {   		// request-callback
							var sql = req.sql, 			// sql connector hash
								query = req.query;		// request query parameters

							sql.query("...", 			// sql query with ?-tokens
								query, 					// query parameters
								function (err,data) {  	// query error and data
									res(err || data);	// return error or data
							});
						}

					The response callback **res** must be called, and must return either
					an **Error** object, a **String** message, an **Array** list of
					records, or an **Object** hash.
			
			#fit.sh
				:markdown
					The flexibility of Bash sh-engines comes with 
					considerable overhead and security implications; for these reasons, sh-engines
					are typically disabled. 

					These engines suffer o(1) second of overhead in loading/compiling a python/nodejs/etc module each time
					the engine is called.  This translates into a 6 hour overhead in a 	typical chipping workflow containing 
					20K chips/footprint.  When, however, workflows can be focused to a small area-of-interest, Bash 
					overhead can be tolerated.

					Bash engines are supported in both the HYDRA and *#{title}* framework.  In
					the HYDRA framework, the engine's script is wrapped in a HYDRA proprietary 
					soapUI (nonrestful XML) interface serviced by HYDRA's web service.  In
					*#{title}* framework, the engine's script is wrapped in a JSON (restful) interface
					serviced by *#{title}*'s web service.  *#{title}*'s service supports workflow engines (to 
					bypass the intrinsic overhead in calling sh-engines), as well as a mechanisms 
					to directly interface with clients and other workflow engines. And whereas 
					*#{title}* is PKI driven, HYDRA is login driven.

					This [sh-shell engine](/demo.db) example (test [here](/demo.db)):

						ls
						python mypgm.py
						
					with initial context: 
						
						{
							"KEY": value, ...
						}

					illustates an engine that simply list the files in the current directory,
					the call the mypgm python module.

			#fit.jade
				:markdown
					Jade engines contain [Jade markdown](/skinguide.view) to manage client
					views.  A Jade engine is invoked at a **view end-point** with optional
					parameters defined by the Jade engine.

			#fit.sq
				:markdown
					This [sql engine](/engine.view?engine=sql&name=demo):

						SQL.select = function (sql,recs,cb) {
							var q = sql.query("SELECT * FROM ?? WHERE ?",["intake",{TRL:2}])
							.on("result", function (rec) {
								rec.Cat = rec.Name + rec.Tech;
								recs.push(rec);
							})
							.on("end", function () {
								Log("returning recs="+recs.length);
								cb(recs);
							});
							Log("sql command="+q.sql);
						}

					with initial context:

							{ key: value, ...}

					is a CRUD-select [simply selects)(/demo.sql) all records from the **intake** 
					dataset whose **TRL** is at 2, and adds a **Cat** field to each record.  Note 
					again that all i/o (here console.log) is sent to the **service** console.  Note 
					too that when this engine is executed (read/GET) for the first time, the engine is 
					simply added to *#{title}*; subsequent executions return the desired records to 
					the client.

			#fit.m
				:markdown
					This [stateless matlab-engine](/mdemo1.run):

						function Save = mdemo1(ctx)
							Save = ctx.a + ctx.b;
						end
						
					returns the sum of its context *a* and *b* keys into its *Save* context key.

			#fit.me
				:markdown
					This [stateless emulated matlab engine](/medemo1.run):
					
						Save = a * (b + a);
						disp(Save);
						
					computes its *Save* context key given its *a* and *b* context keys.
					
					This [example](/engines.view):

						Z = [1,2;3,4];
						X = A * A';
						Y = B * B';

					with context:
					
						"Entry": {
							"A": "SELECT a2,a3,a6 FROM app.MATtest WHERE least(?,1)",
							"B": "SELECT a1,a6 FROM app.MATtest WHERE least(?,1)"
						}

					illustrates how its context variables *A* and *B* are imported with its sql-entry where
					the sql ?-tokens are sourced from the supplied query parameters.
					[For example](/demo1.db?name=test) places *X*, *Y*, and *Z* into its context after
					importing its *A* and *B* context keys.  When an engine terminates,
					it is free to store its context variables into its database with its sql-exit.

					This [emulated matlab example](/engines.view)

						Z = [1,2;3,4];
						X = A * A';
						Y = B * B';
						R = addIt(1,2);

					with context:
					
						"Entry": {
							"A": "SELECT a2,a3,a6 FROM app.MATtest WHERE least(?,1)",
							"B": "SELECT a1,a6 FROM app.MATtest WHERE least(?,1)"
						}

						"Require": {
						   "addIt" :  function (a,b) { return a+b; }
						}
						
					shows how these engines are extended with *Require*.

			#fit.mo
				:markdown
					[Model engines](/engine.view) are used/defined by workflows when 
					systems are referenced/saved from within the [workflow editor](/nodeflow.view).  
					Model engines should remain disabled to prevent execution.

			#fit.cv
				:markdown
					cv-machines learn, locate and classify objects.  The [haar engine](/engine.view?engine=cv&name=haar),
					for example, executes a cv-machine using a context:
					
						size = 50  	// feature size in [m]
						pixels = 512 	// samples across a chip [pixels]
						step = 0.01 	// relative seach step size
						range = 0.1 	// relative search size
						detects = 8		// hits required to declare a detect
						limit = 1e99 	// restrict maximum number of schips to ingest
						test = "test" 	// test case to store results
						scale = [0:1] || 8  		// scale^2 is max number of features in a chip

					which are related to (must cleanup this doc):
					
						{ 
							frame: {
								job: jpg file to load and examine
							},
							detector: {
								scale: 0:1 ,
									//specifies how much the image size is reduced at each image scale step, and thus defines a 
									//scale pyramid during the detection process.  E.g. 0.05 means reduce size by 5% when going to next image 
									//scale step.  Smaller step sizes will thus increase the chance of detecting the features at diffrent scales. 
								delta: 0:1 ,
									//features of dim*(1-delta) : dim*(1+delta) pixels are detected
								dim: integer ,
									//defines nominal feature size in pixels
								hits: integer ,
									//specifies number is required neighboring detects to declare a single detect.  A higher value
									//results in less detections of higher quality. 3~6 is a good value.
								cascade: [ "path to xml file", ... ] ,
									//list of trained cascades
								net: string
									//path to prototxt file used to train caffe cnn	
							}
						}

			#fit.py
				:markdown
					This [stateless py-engine](/pydemo1.run):

						def init(ctx):
							global TLIB
							import testlib as TLIB   # looks for imports under PYTHONPATH

						def pydemo1(ctx):
							print "welcome to python you lazy bird"
							print "test",TLIB.testf(123)

							SQL0.execute("SELECT * from app.Htest", () )
							for (rec) in SQL0:
								print rec

							ctx['Save'] = [ {'x':1, 'y':2, 'z':0}, {'x':3, 'y':4, 'z':10}]

					with initial context:

						"query": {"Name":"test"}

					will display *Htest* data at the service console, then returns *Save* data to its plugin context.  The
					plugin could store data in any CTX key, but the *Save* and *Save_STATE* keys can be used to 
					interface with the master workflow (see Plugins).  Every python engine also has access to the 
					SYS, JSON, LWIP and CAFFE (if on GPU VMs) libs, as well as the SQL0 (read) and SQL1 (write) cursors.
					
					This [stateful python engine](/pydemo2.exe) can be used in a workflow to 
					maximize data stationarity:

						def f1(tau,parm):
							print "in port f1"
							return 0

						def f2(tau,parm):
							print "in port f2"
							return 0

						print "executing on port " + PORT

					with initial context: 

						"query": {"Name":"test"},
						"ports": {
						}

					by defining its i/o ports *f1* / *f2* to be called during the workflow 
					with the incoming *tau* event token and the assoicated *parm* hash taken 
					from the defined *ports* hash.  Although this engine would be typically used
					in [stateful flows](/nodeflow.view), it can also be used in a stateless flow
					by passing *port* (as demonstrated [here](/demo2.py?port=f1)).
					
			#fit.js
				:markdown
					# examples
					
					This [stateless js-engine](/jsdemo1.run):

						function jsdemo1(ctx, res) {
							Log("jsdemo1 ctx", ctx);
							var debug = false;

							if (debug) {
								Log("A="+ctx.A.length+" by "+ctx.A[0].length);
								Log("B="+ctx.B.length+" by "+ctx.B[0].length);
							}

							ctx.Save = [ {u: ctx.M}, {u:ctx.M+1}, {u:ctx.M+2} ];
							res(ctx);

							if (debug)
								$( "D=A*A'; E=D+D*3; disp(entry); ", ctx, (ctx) => {
									Log( "D=", ctx.D, "E=", ctx.E);
								});

					with initial context: 

						"M": 3, 
						"query": {  // default sql-entry query parms if none supplied on url
							"Name": "DefaultTestName"
						},
						"Entry": {
							"A": "SELECT a2,a3,a6 FROM MATtest WHERE least(?,1)",
							"B": "SELECT a1,a6 FROM MATtest WHERE least(?,1)"
						},
						"Exit": {
							"A": "INSERT INTO ?? SET ?"
						}

					will, on entry, prime its *A* and *B* context keys using its *entry* sql-queries: the "?" therein
					references its context *query* hash (as overridden by url query parameters).  On
					exit, its context *B* context variable is exported by its *exit* sql-query.   
					
					This [stateless js-engine](/jsdemo2.exe?name=test):

						function jsdemo2(ctx,res) {
							$("a = inv(X' * X) * X' * y", ctx, (ctx) => {
								Log(ctx);

								var 
									a = ctx.a,
									N = ctx.N = a.length,
									b = $(N, (n,B) => B[n] = a[n]);

								res(ctx);							
							});
						}

					with initial context: 
					
						"M": 3,
						"Entry": {
							"X": "SELECT p0,p1,p2 FROM Htest WHERE least(?,1)",
							"y": "SELECT FPR from Htest WHERE least(?,1)"
						}

					uses the mathjs MATH module to do regression analysis via an emulated Matlab machine.  Here, 
					the **Name**, **Used** and **M** parameters -- acquired from the URL and/or context query -- are 
					used to retrieve data 
					from the **Htest** dataset.  This data is then used to setup a regression companion 
					matrix **X** and measurement vector **y**.  Regression results **a** are then saved into 
					a **b** vector (which, for example, may be saved with a Context.entry sql).

					Whereas the previous engines are stateless -- they do not define functions (i.e. ports) -- this
					[stateful js-engine](/jsdemo3.exe):

						x = 123;
						function fi(tau,parm) {
						  return 0;
						}; 

						function fo(tau,parm) {
						  return 0;
						};

						function jsdemo3(ctx,res) {
						}

					with initial context: 
					
						{ 
							"fi" : {x:10, y:20},
							"fo": {x:11, y:21} 
						}

					defines **fi**, **fo** in its context, as [demonstrated here](/demo3.js).
					
					Subsequent requests at the step/POST endpoint with **port** = "fi" or "fo" will call the
					function with the current **tau** workflow events and the **parm** set to the 
					corresponding "ports" hash.					

					# matrix manipulator
					
					All js-plugins have access to the $( ... ) matrix manipulator providing: matlab-like matrix manipulators, 
					light weight image processing, symbolic algebra, dsp, machine learning, regression, expectation-
					maximization methods, neural and bayseian networks, data i/o, task sharding, and special functions.
					Use $( ... ) as follows:
					
						$( "matlab script", ctx, (ctx) => {   // eval script into context ctx with callback(ctx)
							...
						} );

						$( "matlab script", ctx );   // eval script into context ctx

						var A = $( "matlab expression" );  // eval expression and return resulting matrix

						$( N, (n,A) => A[n] = ... );  // define vector A of N elements

						$( [M,N, ... ], (n,m, ... A) => A[m][n] ... = ... );	// define M x N matrrix

						$( {		//  import functions
							somefn: function (args) { ... },
							:
							:
						} );

						$.somefn( args ); // use imported function

						$( "somefn(args)", ctx );  // use imported function

						$( {  		// execute a simple task sharder
								keys: "i,j,k",  	// e.g. array indecies
								i: [0,1,2,3],  		// domain of index i
								j: [4,8],				// domain of index j
								k: [0],					// domain of index k
								qos: 0,				// regulation time in ms if not zero
								local: false, 		// enable to run task local, i.e. w/o workers and nodes
								workers: 4, 		// limit number of workers (aka cores) per node
								nodes: 3 			// limit number of nodes (ala locales) in the cluster
							}, 

							// here, a simple task that returns a message 
							($) => "my result is " + (i + j*k) + " from " + $.worker + " on "  + $.node,

							// here, a simple callback that displays the task results
							(msg) => console.log(msg) 
						);

						$({  // example that defines a more complex tasker 
							mytask:   // define a mytestfn to be sharded 
								($) => "my result is " + (i + j*k) + " from " + $.worker + " on "  + $.node,

							mysharder:  // simple a task sharding interface
								( iList, jList ) => {  
								$({  		// distribute t/o TOTEM cloud
										keys: "i,j",  	// e.g. array indecies
										i: iList,
										j: jList,
										k: [10,20],
										qos: 0,				// regulation time in ms if not zero
										local: false, 		// enable to run task local, i.e. w/o workers and nodes
										workers: 4, 		// limit number of workers/cores per node
										nodes: 3 			// limit number of nodes/locales in the cluster
									}, 

									// here, a simple task that returns a message 
									$.mytask,

									// here, a simple callback that displays the task results
									(msg) => console.log(msg) 
								);
							}
						});

						$("mysharder( [1:100], [50:200] )");  // execute the sharder
						
						evs.$ || "query".$ ( "group" || "all", (evs) => {  // thread events evs = [ev, ev, ...] to callback(evs || null) 
						
							if (evs)   // there are ingested events

							else 	// events exhausted so
						
						} );  

						evs.$( ctx, (evs) => { ... } );		// save aggregated events to context ctx with callback(unsaved events)

	#accordion.Machines
		:markdown
			All engines rely on a machine MAC = opencv | python | ... as implemented under
			**engine/ifs/MAC/MAC.cpp** and bound to **#{title}** using *node-gyp rebuild* as 
			provided by *maint.sh bind*.  
			
			opencv-machines, for example, implement the following pattern:

				class OPORT { 								 	// output port
					OPORT(str Name, V8OBJECT Parm) { 		// Initiallize 
					};
					// Members follow
				};
				class IPORT { 								 	// input port
					IPORT(str Name, V8OBJECT Parm) { 		// Initiallize 
					};
					// Members follow						
				};
				class FEATURE { 							// Output object
					FEATURE( ... ) {						// Initialize 
					};	
					// Members follow
				};
				class CVMACHINE : public MACHINE {  
					int atch(IPORT &port, V8ARRAY tau) { 	// Latch input context to input port
						return 0; // if successful
					}
					int latch(V8ARRAY tau, OPORT &port) { 	// Latch output port to output context
						return 0; // if successful
					}
					int program (void) { 		// program and step machine
					}
					int call(const V8STACK& args) {  // nodejs interface
					}
					// Members follow
				}

			When bound to **#{title}** (using *node-gyp rebuild* provided by *maint.sh bind*), a **pool** of 
			(typically 256) MAC machines is reserved to run multiple (context independent) compute 
			threads at

				error = MAC.call( [ id string, code string, context hash ] )
				error = MAC.call( [ id string, port string, context hash or event list] )

			this call returning an interger error code (non-zero if a fault occured).

			The thread id (typically "Name.Client.Instance") uniquely identifies the compute thread.  
			Compute threads can be freely added to the pool until the pool becomes full.  

			When stepping a machine, the code string specifies either the name of the engine port on 
			which the arriving context is latched, or the name of the output port on which the departing 
			context is latched; thus stepping the machine in a stateful way (to maximize data restfulness).
			Given, however, an empty code string , the machine is stepped in a stateless way, that is, 
			by latching context to all input ports, then latching all output ports to the context.

	#accordion.Commands
		:markdown
			The following commands maintain **#{title}**:

				GET /ping 		# check client-server connectivity
				GET /alert 		# broadcast alert &msg to all clients
				GET /riddle		# validate client when antibot protection configured
				GET /task 		# shard task to workers and nodes
				GET /help 		# request help from a poc
				GET /ingest		# start source ingest 
				GET /config 	# show system configuration
				GET /service/algorithm/PLUGIN # Execute PLUGIN with SOAP/XML parameters

//- UNCLASSIFIED