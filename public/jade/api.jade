// UNCLASSIFIED

extends site
append site_help
	:markdown
		See also the companion [skinning guide](/skinguide.jade) and the [programmers ref manual](/shares/doc/*#{title}*/index.html).
append site_parms
	- view = "Typical"
	- dock = "top"
append site_body

	#fit.Intro

		:markdown
			As documented in this API, *#{title}* provides ENDPOINTs:

				(select) GET 	 /NODE ?? NODE ...
				(update) PUT 	 /NODE ?? NODE ...
				(insert) POST 	 /NODE ?? NODE ...
				(delete) DELETE /NODE ?? NODE ...

			 to access a NODE:

				DATASET.TYPE ? QUERY ? QUERY ...
				ENGINE.TYPE ? QUERY ? QUERY ...
				FILEPATH.TYPE ? QUERY ? QUERY ...
				COMMAND.TYPE ? QUERY ? QUERY ...
				
			using an optional QUERY:
			
				KEY [OP] = VALUE & ...
				KEY [OP] :  VALUE & ...
			
			where the TYPE will format data:

				db | xml | csv | txt | tab | tree | flat | kml | encap | html | json | geojson

			render a [view/skin](/skinguide.view):

				view | pivot | site | spivot | brief | gridbrief | pivbrief | run | plugin | runbrief | calc

			execute, return usecase results, or list code of an engine:

				exe | USECASE | js | py | ma | ...

			return file attributes:

				delta | nav | stat | ...
				
			or generate office files:

				xpdf | xjpg | xgif | xdoc | xppt | xxls | ...

	#accordion.Views

		#fit.Endpoints

			:markdown
				Views are [editable](/edit.view) [Jade skinning engines](/skinguide.view) that defines 
				a client's view, and are accessed at the following routes:

					GET	/SKIN.view	Render SKIN from enabled engines or internal jade files
					GET	/DATASET.view	Dynamically generate a skin for this DATASET

		#fit.Parameters
			:markdown
				Views parameters are SKIN dependent. For example, [plot.view?help](/plot.view?help) will 
				generate help for the *plot.view*.

		#fit.Examples
			:markdown
				[view plot help](/plot.view?help)  
				[view the news dataset](/news.view)  
				[view the jsdemo1 plugin](/jsdemo1.run)
				[view the skinning guide](/skinguide.view)  
				[view the api](/api.view)  
				[view a sample application](/swag.view)  
				[view models](/flow.view)  
				[view a sample briefing](/home_brief.view)  
				[view briefing under the ELT1 area](/ELT1.home_brief.view)

	#accordion.Files
		#fit.Endpoints
			:markdown
				File access is provided at the following endpoints:

					GET	/AREA/FILE.TYPE?QUERY	Return FILE from AREA using query parameters
					GET	/ATTR/FILE.TYPE?QUERY	Return ATTRribute of a FILE
					GET	/AREA/	Return list of files in this AREA

				where AREA references a #{nick} internal file store or a one of its virtual stores:

					code	file pulled from the [engines db](/engine.view)
					jade	file pulled from the [engines db](/engine.view) or (failing that) the jade file area
					uploads	file pulled from delete-on-access area
					stores	file pulled from long-term area
					positives	file pulled from positive-proof area
					negatives	file pulled from negative-proof area
					cert	generates a PKI cert for the requested user
					sim		reserved for simulation engines
					chips	image chipping cache
					tips	image tipping cache
					shares 	spot to place skinning content
					
		#fit.Parameters
			:markdown
				File retrieval parameters include:

					_has	find best file by string containment 
					_nlp	find best file by nlp context 
					_bin	find best file by binary expression 
					_qex	find best file by query expansion 
					_score	minimum score required

		#folder.Readers
			#fit.Intro
				:markdown
					Readers are builtin [engines](/engine.view) that automatically index a variety of 
					document, graphics, presentation, and spreadsheet files when uploaded into
					**#{nick}**.  Ingested text is checked for readibility, indexed to the best
					using [NLP training rules](/admins.view), then reflected into the [file stores](/files.view).

			#fit.Special
				code.
					html	- Web site
					rss	- News feed
					idop	- NTM imagery

			#fit.Document
				code.
					bib      - BibTeX [.bib]
					doc      - Microsoft Word 97/2000/XP [.doc]
					doc6     - Microsoft Word 6.0 [.doc]
					doc95    - Microsoft Word 95 [.doc]
					docbook  - DocBook [.xml]
					docx     - Microsoft Office Open XML [.docx]
					docx7    - Microsoft Office Open XML [.docx]
					fodt     - OpenDocument Text (Flat XML) [.fodt]
					html     - HTML Document (OpenOffice.org Writer) [.html]
					latex    - LaTeX 2e [.ltx]
					mediawiki - MediaWiki [.txt]
					odt      - ODF Text Document [.odt]
					ooxml    - Microsoft Office Open XML [.xml]
					ott      - Open Document Text [.ott]
					pdb      - AportisDoc (Palm) [.pdb]
					pdf      - Portable Document Format [.pdf]
					psw      - Pocket Word [.psw]
					rtf      - Rich Text Format [.rtf]
					sdw      - StarWriter 5.0 [.sdw]
					sdw4     - StarWriter 4.0 [.sdw]
					sdw3     - StarWriter 3.0 [.sdw]
					stw      - Open Office.org 1.0 Text Document Template [.stw]
					sxw      - Open Office.org 1.0 Text Document [.sxw]
					text     - Text Encoded [.txt]
					txt      - Text [.txt]
					uot      - Unified Office Format text [.uot]
					vor      - StarWriter 5.0 Template [.vor]
					vor4     - StarWriter 4.0 Template [.vor]
					vor3     - StarWriter 3.0 Template [.vor]
					xhtml    - XHTML Document [.html]

			#fit.Graphics
				code.
					bmp      - Windows Bitmap [.bmp]
					emf      - Enhanced Metafile [.emf]
					eps      - Encapsulated PostScript [.eps]
					fodg     - OpenDocument Drawing (Flat XML) [.fodg]
					gif      - Graphics Interchange Format [.gif]
					html     - HTML Document (OpenOffice.org Draw) [.html]
					jpg      - Joint Photographic Experts Group [.jpg]
					met      - OS/2 Metafile [.met]
					odd      - OpenDocument Drawing [.odd]
					otg      - OpenDocument Drawing Template [.otg]
					pbm      - Portable Bitmap [.pbm]
					pct      - Mac Pict [.pct]
					pdf      - Portable Document Format [.pdf]
					pgm      - Portable Graymap [.pgm]
					png      - Portable Network Graphic [.png]
					ppm      - Portable Pixelmap [.ppm]
					ras      - Sun Raster Image [.ras]
					std      - OpenOffice.org 1.0 Drawing Template [.std]
					svg      - Scalable Vector Graphics [.svg]
					svm      - StarView Metafile [.svm]
					swf      - Macromedia Flash (SWF) [.swf]
					sxd      - OpenOffice.org 1.0 Drawing [.sxd]
					sxd3     - StarDraw 3.0 [.sxd]
					sxd5     - StarDraw 5.0 [.sxd]
					sxw      - StarOffice XML (Draw) [.sxw]
					tiff     - Tagged Image File Format [.tiff]
					vor      - StarDraw 5.0 Template [.vor]
					vor3     - StarDraw 3.0 Template [.vor]
					wmf      - Windows Metafile [.wmf]
					xhtml    - XHTML [.xhtml]
					xpm      - X PixMap [.xpm]

			#fit.Presentation
				code.
					bmp      - Windows Bitmap [.bmp]
					emf      - Enhanced Metafile [.emf]
					eps      - Encapsulated PostScript [.eps]
					fodp     - OpenDocument Presentation (Flat XML) [.fodp]
					gif      - Graphics Interchange Format [.gif]
					html     - HTML Document (OpenOffice.org Impress) [.html]
					jpg      - Joint Photographic Experts Group [.jpg]
					met      - OS/2 Metafile [.met]
					odg      - ODF Drawing (Impress) [.odg]
					odp      - ODF Presentation [.odp]
					otp      - ODF Presentation Template [.otp]
					pbm      - Portable Bitmap [.pbm]
					pct      - Mac Pict [.pct]
					pdf      - Portable Document Format [.pdf]
					pgm      - Portable Graymap [.pgm]
					png      - Portable Network Graphic [.png]
					potm     - Microsoft PowerPoint 2007/2010 XML Template [.potm]
					pot      - Microsoft PowerPoint 97/2000/XP Template [.pot]
					ppm      - Portable Pixelmap [.ppm]
					pptx     - Microsoft PowerPoint 2007/2010 XML [.pptx]
					pps      - Microsoft PowerPoint 97/2000/XP (Autoplay) [.pps]
					ppt      - Microsoft PowerPoint 97/2000/XP [.ppt]
					pwp      - PlaceWare [.pwp]
					ras      - Sun Raster Image [.ras]
					sda      - StarDraw 5.0 (OpenOffice.org Impress) [.sda]
					sdd      - StarImpress 5.0 [.sdd]
					sdd3     - StarDraw 3.0 (OpenOffice.org Impress) [.sdd]
					sdd4     - StarImpress 4.0 [.sdd]
					sxd      - OpenOffice.org 1.0 Drawing (OpenOffice.org Impress) [.sxd]
					sti      - OpenOffice.org 1.0 Presentation Template [.sti]
					svg      - Scalable Vector Graphics [.svg]
					svm      - StarView Metafile [.svm]
					swf      - Macromedia Flash (SWF) [.swf]
					sxi      - OpenOffice.org 1.0 Presentation [.sxi]
					tiff     - Tagged Image File Format [.tiff]
					uop      - Unified Office Format presentation [.uop]
					vor      - StarImpress 5.0 Template [.vor]
					vor3     - StarDraw 3.0 Template (OpenOffice.org Impress) [.vor]
					vor4     - StarImpress 4.0 Template [.vor]
					vor5     - StarDraw 5.0 Template (OpenOffice.org Impress) [.vor]
					wmf      - Windows Metafile [.wmf]
					xhtml    - XHTML [.xml]
					xpm      - X PixMap [.xpm]

			#fit.Spreadsheet
				code.
					csv      - Text CSV [.csv]
					dbf      - dBASE [.dbf]
					dif      - Data Interchange Format [.dif]
					fods     - OpenDocument Spreadsheet (Flat XML) [.fods]
					html     - HTML Document (OpenOffice.org Calc) [.html]
					ods      - ODF Spreadsheet [.ods]
					ooxml    - Microsoft Excel 2003 XML [.xml]
					ots      - ODF Spreadsheet Template [.ots]
					pdf      - Portable Document Format [.pdf]
					pxl      - Pocket Excel [.pxl]
					sdc      - StarCalc 5.0 [.sdc]
					sdc4     - StarCalc 4.0 [.sdc]
					sdc3     - StarCalc 3.0 [.sdc]
					slk      - SYLK [.slk]
					stc      - OpenOffice.org 1.0 Spreadsheet Template [.stc]
					sxc      - OpenOffice.org 1.0 Spreadsheet [.sxc]
					uos      - Unified Office Format spreadsheet [.uos]
					vor3     - StarCalc 3.0 Template [.vor]
					vor4     - StarCalc 4.0 Template [.vor]
					vor      - StarCalc 5.0 Template [.vor]
					xhtml    - XHTML [.xhtml]
					xls      - Microsoft Excel 97/2000/XP [.xls]
					xls5     - Microsoft Excel 5.0 [.xls]
					xls95    - Microsoft Excel 95 [.xls]
					xlt      - Microsoft Excel 97/2000/XP Template [.xlt]
					xlt5     - Microsoft Excel 5.0 Template [.xlt]
					xlt95    - Microsoft Excel 95 Template [.xlt]
				
		#fit.Examples
			:markdown
				[download a file from the shares area](/shares/welcome.pdf)  
				[return flare json file from data area](/data/flare.json)

	#accordion.Datasets
		#fit.Endpoints
			:markdown
				Both real and virtual datasets are reached at the following endpoints:

					GET	/DATASET.TYPE		Return data from DATASET
					PUT	/DATASET.TYPE		Update DATASET with body parameters
					POST	/DATASET.TYPE	Insert body parameters into DATASET
					DELETE	/DATASET.TYPE	Delete from DATASET 

		#fit.Parameters
			:markdown
				The #{nick} recognizes the following parameters.

				# Query parameters

					KEY = value 		// matching test
					KEY = [ ] 				// null test
					KEY = [ min , max ] // inclusive range (if unsafe queries enabled)
					KEY = [ x, y, z, ... ] // bounding box (reserved)
					A.KEY = B.KEY 	// Join datasets A and B on specified KEYs (experimental)
					expression 			// e.g. KEY<value, KEY>value,... (if unsafe queries enabled)

				# Body and File Upload parameters

				JSON formatted parameters, or a file upload script following this convention:

					key = val ; key = val ; ...  [cr newline]
					:
					:
					file data [cr newline]

				# Searching parameters

					_has	= search records by containment over fulltext keys
					_nlp	= search records by nlp context over fulltext keys
					_bin	= search records by binary expression over fulltext keys
					_qex	= search records by query expansion over fulltext keys
					_score	= minimum search score required
					_save = name to save current url

				# Indexing and Grouping parameters

					_pivot	= KEY,KEY,... pivot records on KEYs with NodeID = "ID,ID,..." groups
					_browse	= KEY,KEY,... browse records on KEYs with NodeID = "name/name/ ..." 
					_group	= KEY,KEY,... group records on KEYs
					_index	= KEY,KEY,... return only specified KEYs
					_sort	= KEY,KEY,... sort records ascending on KEYs
					_sort	= [{property:KEY,sort:DIRECT}, ...] sort records asc|desc on KEYs 
					_geo = KEY,KEY,... add geojson geometry keys named gKEY to the records

				# Conversion parameters

					_queue	= KEY for queuing record state
					_json	= KEY,KEY,... json parse selected KEYs and index with KEY=IDX.IDX...
					_blog	= KEY,KEY,... markdown KEYs containing tags = [image | post | update | link | SKIN, W, H, DS?QUERY]( /url | PARM=ds.KEY expression; ...)
					_nav	= open | tree | rename | size ,root/A/B/... folder navigation 

				# Record parameters

					_client	= name of client to broadcast to other clients
					_lock	= enable record locking
					_view 	= name of view to correlate with dataset
					_src	= override dataset path 
					_limit	= number of records to return
					_start	= record position to start returning records

				# Legacy parameter
					_page	= page number (reserved)
					_mark	= KEY,KEY,... markdown these KEYs 
					_jade	= KEY,KEY,... jadeify these KEYs (experimental - use _kjade)
					_kjade	= KEY,KEY,... jadeify these KEYs (to be retired)

		#fit.Examples
			:markdown
				[return first 20 test records having x=123 and y is null sorted by u and b](/test.db?_start=0&_limit=10&x=123&y=null)   
				[return test records having x=123 and order by u and v](/test.db?sort=[{"property":"u","direction":"asc"},{"property":"v","direction":"asc"}]&x=123)   
				[return parms records](/parms.db)  
				[return parms records having specified parm](/parms.db?Parm=Band)  
				[return news records within certain age range](/news.db?age=690:693)  					
				[insert test record x=123,y=null](POST/test.db?x=123&y=null})  
				[update test record ID=10 with x=123,y=null ](PUT/test.db?x=123&y=null&ID=10)  
				[delete test record ID=10](DELETE/test.db?ID:10)  
				[return intake records](/intake.db)  
				[return intake records pivoted by TRL and Ver](/intake.tree?_group=TRL,VER)

		#fit.Virtual.Datasets
			#grid.DS.Parms(
				path="/parms.db",crush,
				head="Print,Help",
				cols="ID.a,Parm,Label,Type,Special,By Inspect.c,By Analysis.c,By Demo.c")

				:markdown
					The [parameter list](/parms.db) dataset defines how dataset fields are exposed to end 
					clients in grids, forms, folders, etc used in **#{nick}** skins.  To each field corresponds a label name, verification 
					methods, and access priviledges.

			#grid.Roles(
				path="/roles.db",crush,
				head="Print,Help",
				cols="Table.T,Special.H,INSERT.T,UPDATE.T,DELETE.T,SELECT.T,IMPORT.T,EXPORT.T")

				:markdown
					The [user roles](/roles.db) dataset defines the roles assumed when clients insert, update, delete, and 
					select records from a specific dataset. 

			#grid.Intrinsic(
				path="/TABLES.db",crush,
				head="Print,Help",
				cols="Name.h")

				:markdown
					The [TABLES](/TABLES.db) provides a list of **#{nick}** datasets.

			#grid.Admin(
				path="/ADMIN.db",crush,
				head="Print,Help",
				cols="TABLE_NAME,TABLE_TYPE,ENGINE,VERSION,ROW_FORMAT,TABLE_ROWS,AVG_ROW_LENGTH,DATA_LENGTH,MAX_DATA_LENGTH,CREATE_TIME,UPDATE_TIME,TABLE_COMMENT")

				:markdown
					The [ADMIN](/ADMIN.db) provides detailed storage and technical information on all **#{nick}** datasets.

			#grid.Sys.Config(
				path="/CONFIG.db",crush,
				head="Print,Help",
				cols="classif,extnet,disk,cpu,cpuat,platform,totalmem,freemem,uptime,cpus,host,netif,temp")

				:markdown
					System configuration information is available at [CONFIG](/CONFIG.db).

			:markdown
				The **#{nick}** supports *Virtual Datasets* through its CRUDE (*create*, *read*, *update*, *delete*, 
				and *execute*) interface.  This CRUDE interface governs both *Database Datasets* and *Virtual Datasets*.
				For example, an *execute* on dataset X will typically import/export the data to/from dataset 
				X as controlled by its associated query parameters, a list of which might be returned by supplying a 
				&help parameter.  Below are several important **Virtual datasets**:

				+	[return upload files](/uploads.FILES.db), [stores files](/stores.FILES.db), etc
				+	[return jade views](/VIEWS.db)  
				+	[return connected users](/USERS.db)  
				+	[return engine summary](/ENGINES.db)  
				+	[return queue summary](/QUEUES.db)  
				+	[return work cliques](/CLIQUES.db)  
				+	[return system health](/HEALTH.db)  
				+	[return database activity](/ACTIVITY.db)  
				+	[return system configuration](/CONFIG.db)  
				+	[flatten searchable tables](/CATALOG.execute) for [search catalog](/CATALOG.db)  
				+	[contengency data](/ROCSUM.db)  
				+	[update](/events.execute) work predication [events](/events.db) and [stats](/jobstats.db)  
				+	[reflect git change logs](/issues.execute) to [tracked issues](/issues.db)  
				+	[broadcast messages](/sockets.execute) to [connected users](/sockets.db)  
				+	[import milestones](/milestone.execute) from internal spreadsheet

	#accordion.Plugins
		#fit.Endpoints
			:markdown
				A plugin -- a dataset-engine pair -- is accessed from the following endpoints:

					GET /PLUGIN.exe?id=CASE 	Run PLUGIN engine with id-specified CASE context
					GET /PLUGIN.exe?name=CASE 	Run PLUGIN engine with name-specified CASE context
					GET /PLUGIN.exe?QUERY 	Run PLUGIN engine with QUERY context
					GET /PLUGIN.view	View and execute selected PLUGIN usecases
					GET /PLUGIN.run		View, execute, edit engine and manage PLUGIN job queues

				An engine's usecase *context* can be agumented by optional keys:  
				> [Export+](/#{ds}.add?Export=false) [/-](/#{ds}.sub?Export) writes engine results into a file  
				> [Ingest+](/#{ds}.add?Ingest=false) [/-](/#{ds}.sub?Ingest) ingests engine results into the database  
				> [Share+](/#{ds}.add?Share=false) [/-](/#{ds}.sub?Share) saves engine results to the status area  
				> [Pipe+](/#{ds}.add?Pipe=doc) [/-](/#{ds}.sub?Pipe) regulates chips and events to the engine  
				> [Description+](/#{ds}.add?Description=doc) [/-](/#{ds}.sub?Description) documents a usecase  
				> [Config+](/#{ds}.add?Config=doc) [/-](/#{ds}.sub?Config) defines usecase context with a js-engine  
				> [Save+](/#{ds}.add?Save=doc) [/-](/#{ds}.sub?Save) aggregates engine results not captured in the Save_KEYs  
				> [Save_KEY](/#{ds}.run?options=more) aggregates engine results [{at:"KEY", ...}, ...]  
					
				When executed, an engine's context is automatically extended with the following keys:  
				> _Host name of plugin  
				> _File current file during workflow  
				> _Voxel current voxel being procesed  
				> _Collects sensor collects made under current voxel  
				> _Chip  filepath for first jpeg collect under current voxel  
				> _Flux solar flux at earth's surface under current voxel  
				> _Load event query for current voxel  

				An engine's *Pipe* context key contains job regulation parameters:  
				> file: "/FILTER?QUERY" || "PLUGIN.CASE" || "uploaded FILENAME" || "FILE.jpg" || "FILE.json"  
				> group: "KEY,..." || ""  
				> where: { KEY: VALUE, ...} || {}  
				> order: "KEY,..." || "t"  
				> limit: VALUE || 1000  
				> task: "NAME" || ""  
				> aoi: "NAME" || [ [lat,lon], ... ] || []

				and where the *Description* key may contain [blogging tags](/skinguide.view) of the form:
				> \[ VIEW ; WIDTH ; HEIGHT ]( DS ? PARM ; PARM ; ... )  
				> \[ LINK ]( URL )   
				> !$ inline TeX $  ||  $$ break TeX $$ || a$ AsciiMath $ || m$ MathML $  
				> ${ KEY } || $${ TeX matrix KEY } || ${ KEY }( SHORTCUT ) || !${ EXPR }  
				> #tag
				
				to document usecases.  For example:

					[plot;100;200](x=Save$a;ys=Save$b$c)
					[post;500;100](/home.view)
					[go here grasshopper](https:/go.here)
					[image](/shares/a1.jpg)
					[force;100;100](/queues?_pivots=class)
					$$ \alpha = 1 + \beta $$ impressive 'eh

				to embed: (1) a plot of the requested x,y data using the [d3 plot](/plot.view), (2) the 
				[home.view](/home.view), (3) a link to the url, (4) the [image](/shares.jpg), (5) a
				force graph of the queues dataset pivoted by class using the [d3 force](/force.view) 
				into a frame of specified W x H dimensions, and (6) a TeX equation.

		#fit.Publishing
			:markdown
				Engines placed into #{nick}'s [content folders](map drive #{info.content})/TYPE/PLUGIN.js
				are published to [#{nick}'s engines](/engines.view) when #{nick} starts.  Each discoverd
				TYPE/NAME.js must follow this pattern:
				
					module.exports = {
						clear: true || false,		// enable to drop then recreate the usecases 
						modify: {  // change sql types of existing keys, e.g. MyFloatParm: "int(11)"
							KEY: "sql type", ...
						},
						usecase: {  // add sql keys of specified types, e.g. MyJsonParm: "json"
							KEY: "sql type", ...
						},
						engine: function NAME() { ... } || "def NAME: ..." || ... ,  // engine code
						smop: "path source" // path to m-engine for converting to python"
					}
			
		#fit.Examples
			:markdown
				js ex1 [ui](/jsdemo1.run) [run usecase test1](/jsdemo1.exe?Name=test1) [get usecases](/jsdemo1)
				js ex2 [ui](/jsdemo2.run) [run usecase test1](/jsdemo2.exe?Name=test1) [get usecases](/jsdemo2)
				py ex1 [ui](/pydemo1.run) [run usecase test1](/pydemo1.exe?Name=test1) [get usecases](/pydemo1)
				py ex2 [ui](/pydemo2.run) [run usecase test1](/pydemo2.exe?Name=test1) [get usecases](/pydemo2)
				ma ex1 [ui](/mademo1.run) [run usecase test1](/mademo1.exe?Name=test1) [get usecases](/mademo1)
				ma ex2 [ui](/mademo2.run) [run usecase test1](/mademo2.exe?Name=test1) [get usecases](/mademo2)
	
	#accordion.Agents
		#fit.Endpoints
			:markdown
				Engines can be outsourced to an agent at:

					GET /ENGINE.exe?agent=AGENT&poll=N&QUERY
					GET /ENGINE.exe?agent=AGENT&QUERY

				where the &poll request will start a job, then poll the AGENT every N seconds for its results.
				Conversely, a poll-less request will start a job, then reply on the AGENT to claim the job.  In
				either case, QUERY defines the ENGINE args.

				*#{title}* reciprocates by providing its own AGENT at its **/agent** endpoint.

				Agents provide a means to outsource an engine, while retaining a thread
				on each agent request.  A valid agent must provide *#{title}* the
				following push/pull endpoints:

					GET http://AGENT?push=*#{title}*.CLIENT.ENGINE.ID&args=JSON
					GET http://AGENT?pull=JOBID

				where the push endpoint defines the job being sent to the AGENT (with the CLIENT 
				requesting the agent, the ENGINE being outsourced, the ID of the test case,
				and the JSON args to the ENGINE).  The AGENT eventually responds at 
				the pull endpoint with a JOBID, or a "" if no job could be created.  *#{title}* will periodically 
				poll the agent for the results of JOBID.

		#fit.Examples
		
	#accordion.Engines

		#fit.Endpoints
			:markdown
				Simulation engines are available at these endpoints:

					GET /ENGINE.exe 	Compile and step ENGINE in a stateless workflow
					PUT /ENGINE.exe 	Compile ENGINE in a stateful workflow
					POST /ENGINE.exe	Step ENGINE  in a stateful workflow
					DELETE /ENGINE.exe	Free ENGINE from a stateful workflow

				While the GET-endpoint provides a stateless interface to an engine and its context, the PUT-, POST-,
				and DELETE-endpoints provide a stateful interface to engines which are useful in [workflows](/nodeflow.view).
				Whereas stateless engines (being memoryless) are initialized at each step, stateful 
				engines are initialized only when the workflow is initialized, and typically define i/o ports to
				latch input/output events, thus maximizing data stationarity in a workflow.

				Please know 
				that **#{nick}** does not provide an interactive development environment; engines should be 
				thoroughly debugged in their native development environment before being [inserted](/engines.view) 
				into **#{nick}**.  

		#fit.Parameters
			:markdown
				An engine's inital context is held in its context JSON store:

					{
						"query": { "KEY": value, ... },
						"entry": { "KEY": "select ...", ... } || "select ...",
						"exit": { "KEY": "update ...", ... } || "update ...",
						"KEY": value, 
						"KEY": value, ...						
					}

				On entry, its context is primed using its *entry* sql-queries.  On exit, its context keys 
				can be exported by its *exit* sql-queries.   The "?" in sql-queries references the 
				context *query* (as overridden by url query parameters).  The engine's content is also
				provided an OS, JSON, and an SQL module.

		#folder.Examples(dock="left")
			#fit.url
				:markdown
					URL engines (experimental) provide a simple method to re-route a request
					based on URL parameters, ${req.PARM} and ${plugin.FN(req)} tags; such engines
					follow the pattern:

						"http://URL?PARM=${req.PARM}&PARM=${plugin.FN(req)}"
						
					with initial context:
							{ fn1: function (req) {...}, fn2: ... }

					For example, a **testeng** engine:

						"http://www.someservice.com/ogc?service=wfs&layer=${req.id+"xxx"}&arg=${plugin.angproj(req)}"
						
					with initial context:
					
						{ angproj: function (req) { 
							return Math.cos(req.arg*Math.PI/180); 
							} }

					would reroute "/testeng.xml?id=abc123&arg=45" to "http://www.someservice.com?service=wfs&layer=abc123xxx&arg=0.70710",
					then return someservice's response in xml format.

			#fit.R
				:markdown
					R engines are not yet implemented.

			//
				#fit.Proxy
				:markdown
					Proxy engines are [engines](/engine.view) that run periodically given a
					run interval Period parameter.  A 0-period proxy is run once at **#{nick}** startup.
					The [job hawkers](/admin.view) are proxy engines.
			
				#fit.Virtual
				:markdown
					Virtual-dataset engines are JS-engines that can be accessed and customized
					ny right-clicking its corresponding action button from within a view.  Virtual dataset
					engines follow this pattern:

						Code:
							function (req, res) {   		// request-callback
								var sql = req.sql, 			// sql connector hash
									query = req.query;		// request query parameters

								sql.query("...", 			// sql query with ?-tokens
									query, 					// query parameters
									function (err,data) {  	// query error and data
										res(err || data);	// return error or data
								});
							}
						Context: {
							"KEY": value, ...
						}

					The response callback **res** must be called, and must return either
					an **Error** object, a **String** message, an **Array** list of
					records, or an **Object** hash.
			
				#fit.em
				:markdown
					This [emulated matlab](/emdemo1.exe?name=test) example:

						Code:
							Z = [1,2;3,4];
							X = A * A';
							Y = B * B';

						Context: {
							"entry": {
								"A": "SELECT a2,a3,a6 FROM app.MATtest WHERE least(?,1)",
								"B": "SELECT a1,a6 FROM app.MATtest WHERE least(?,1)"
							}
						}

					illustrates how its context variables *A* and *B* are imported with its sql-entry where
					the sql ?-tokens are sourced from the supplied query parameters.
					[For example](/demo1.db?name=test) places *X*, *Y*, and *Z* into its context after
					importing its *A* and *B* context keys.  When an engine terminates,
					it is free to store its context variables into its database with its sql-exit.

					This [emulated matlab](/emdemo2.exe?name=test)

						Code:
							Z = [1,2;3,4];
							X = A * A';
							Y = B * B';
							R = addIt(1,2);

						Context: {
							"entry": {
								"A": "SELECT a2,a3,a6 FROM app.MATtest WHERE least(?,1)",
								"B": "SELECT a1,a6 FROM app.MATtest WHERE least(?,1)"
							},

							"require": {
							   "addIt" :  function (a,b) { return a+b; }
							}
						}
						
					shows how these engines are extended with the **require** hash.

			#fit.sh
				:markdown
					The flexibility of Bash sh-engines comes with 
					considerable overhead and security implications; for these reasons, sh-engines
					are typically disabled. 

					These engines suffer o(1) second of overhead in loading/compiling a python/nodejs/etc module each time
					the engine is called.  This translates into a 6 hour overhead in a 	typical chipping workflow containing 
					20K chips/footprint.  When, however, workflows can be focused to a small area-of-interest, Bash 
					overhead can be tolerated.

					Bash engines are supported in both the HYDRA and **#{nick}** framework.  In
					the HYDRA framework, the engine's script is wrapped in a HYDRA proprietary 
					soapUI (nonrestful XML) interface serviced by HYDRA's web service.  In
					**#{nick}** framework, the engine's script is wrapped in a JSON (restful) interface
					serviced by **#{nick}**'s web service.  **#{nick}**'s service supports workflow engines (to 
					bypass the intrinsic overhead in calling sh-engines), as well as a mechanisms 
					to directly interface with clients and other workflow engines. And whereas 
					**#{nick}** is PKI driven, HYDRA is login driven.

					This [sh-shell engine](/demo.db) example (test [here](/demo.db)):

						ls
						python mypgm.py
						
					with initial context: 
						
						{
							"KEY": value, ...
						}

					illustates an engine that simply list the files in the current directory,
					the call the mypgm python module.

			#fit.jade
				:markdown
					Jade engines contain [Jade markdown](/skinguide.view) to manage client
					views.  A Jade engine is invoked at a **view end-point** with optional
					parameters defined by the Jade engine.

			#fit.sq
				:markdown
					This [sql engine](/engine.view?engine=sql&name=demo):

						SQL.select = function (sql,recs,cb) {
							var q = sql.query("SELECT * FROM ?? WHERE ?",["intake",{TRL:2}])
							.on("result", function (rec) {
								rec.Cat = rec.Name + rec.Tech;
								recs.push(rec);
							})
							.on("end", function () {
								LOG("returning recs="+recs.length);
								cb(recs);
							});
							LOG("sql command="+q.sql);
						}

					with initial context:

							{ key: value, ...}

					is a CRUD-select [simply selects)(/demo.sql) all records from the **intake** 
					dataset whose **TRL** is at 2, and adds a **Cat** field to each record.  Note 
					again that all i/o (here console.log) is sent to the **service** console.  Note 
					too that when this engine is executed (read/GET) for the first time, the engine is 
					simply added to **#{nick}**; subsequent executions return the desired records to 
					the client.

			#fit.ma
				:markdown
					This [stateless matlab-engine](/mademo1.exe):

						function mademo1(ctx,res)
							ctx.Save = ctx.a + ctx.b;
							res(ctx);
						end

					with initial context: 

						{
						}

					will return the sum of its context *a* and *b* keys.
				
			#fit.mo
				:markdown
					[Model engines](/engine.view) are used/defined by workflows when 
					systems are referenced/saved from within the [workflow editor](/nodeflow.view).  
					Model engines should remain disabled to prevent execution.

			#fit.cv
				:markdown
					cv-machines learn, locate and classify objects.  The [haar engine](/engine.view?engine=cv&name=haar),
					for example, executes a cv-machine using a context:
					
						size = 50  	// feature size in [m]
						pixels = 512 	// samples across a chip [pixels]
						step = 0.01 	// relative seach step size
						range = 0.1 	// relative search size
						detects = 8		// hits required to declare a detect
						limit = 1e99 	// restrict maximum number of schips to ingest
						test = "test" 	// test case to store results
						scale = [0:1] || 8  		// scale^2 is max number of features in a chip

					which are related to (must cleanup this doc):
					
						{ 
							frame: {
								job: jpg file to load and examine
							},
							detector: {
								scale: 0:1 ,
									//specifies how much the image size is reduced at each image scale step, and thus defines a 
									//scale pyramid during the detection process.  E.g. 0.05 means reduce size by 5% when going to next image 
									//scale step.  Smaller step sizes will thus increase the chance of detecting the features at diffrent scales. 
								delta: 0:1 ,
									//features of dim*(1-delta) : dim*(1+delta) pixels are detected
								dim: integer ,
									//defines nominal feature size in pixels
								hits: integer ,
									//specifies number is required neighboring detects to declare a single detect.  A higher value
									//results in less detections of higher quality. 3~6 is a good value.
								cascade: [ "path to xml file", ... ] ,
									//list of trained cascades
								net: string
									//path to prototxt file used to train caffe cnn	
							}
						}

			#fit.py
				:markdown
					This [stateless py-engine](/pydemo1.run):

						def pydemo1(ctx,res):
							print "welcome to python you lazy bird"
							ctx['Save'] = [ {'x':1, 'y':2, 'z':0}, {'x':3, 'y':4, 'z':10}]
							res(ctx)
							if os:
								SQL0 = OS['SQL0']
								SQL0.execute("SELECT * from app.Htest", () )
								#SQL0.execute("SELECT 1 as x, 2 as y", () )
								for (Rec) in SQL0:
									print Rec

					with initial context:

						{
							"query": {"Name":"test"}
						}

					will display *Htest* data at the service console, then returns *Save* data to its plugin context.
					Python engines are also provided an *OS* dictionary containing: an *SQL0* read cursor, 
					an *SQL1* write cursor, the *LWIP* light-weight-image-processing PIL, the JSON 
					encoder/decoder, the *GET_[byStep|byDepth|bulk|discard|...](ctx,cb)* event getters,
					and the	*GET_load(flush,ctx,cb)* event getter.

					This [stateful python engine](/pydemo2.exe) can be used in a workflow to 
					maximize data stationarity:

						def f1(tau,parm):
							print "in port f1"
							return 0

						def f2(tau,parm):
							print "in port f2"
							return 0

						def pydemo2(ctx):
							print "executing on port " + PORT

					with initial context: 

						{
							"query": {"Name":"test"},
							"ports": {
							}
						}

					by defining its i/o ports *f1* / *f2* to be called during the workflow 
					with the incoming *tau* event token and the assoicated *parm* hash taken 
					from the defined *ports* hash.  Although this engine would be typically used
					in [stateful flows](/nodeflow.view), it can also be used in a stateless flow
					by passing *port* (as demonstrated [here](/demo2.py?port=f1)).
					
			#fit.js
				:markdown
					js-engines are automatically supplied the
					[LWIP(lightweight imaging)](https://github.com/EyalAr/lwip),
					[SQL(jsdb-extended database connector)](https://www.npmjs.com/package/mysql), 
					[JSON(encoder-decoder)](https://nodejs.org/docs/latest/api/json), 
					[ME(jslab-extended matlab emulator)](https://www.npmjs.com/package/mathjs),
					[TASK(task sharding)](https://github.com/ACMESDS/jslab),
					[GET(data getter)](https://github.com/ACMESDS/jslab)
					[RAN(markov random process)](https://github.com/ACMESDS/randpr),
					[CRYPTO(crypto)](https://nodejs.org/docs/latest/api/crypto), and
					[MAIL(simple mail transfer)](https://www.npmjs.com/package/nodemailer)
					modules.  The matlab emulator module ME is supported by the
					[DSP(digitial signal processing)](https://www.npmjs.com/package/dsp), 
					[BAYES(bayes network interference )](https://www.npmjs.com/package/jsbayes),
					[GAMMA(standard gamma function)](https://www.npmjs.com/package/gamma),
					[SVD(singular value decomposition)](https://www.npmjs.com/package/node-svd),
					[HMM(hidden markov model solver)](https://www.npmjs.com/package/nodehmm),
					[MLE(maximum liklihood estimator)](https://www.npmjs.com/package/node-svd),
					[MVN(normal multivariates](https://www.npmjs.com/package/multivariate-normal),
					[NRAP(newton-raphson root finder)](https://www.npmjs.com/package/newton-raphson),
					[ZETA(riemann-zeta function](https://www.npmjs.com/package/math-riemann-zeta), and
					[RNN(recurrent nural netroc](https://www.npmjs.com/package/recurrentjs)
					modules.

					This [stateless js-engine example](/jsdemo1.run):

						function jsdemo1(ctx, res) {
							LOG("jsdemo1 ctx", ctx);
							LOG("A="+ctx.A.length+" by "+ctx.A[0].length);
							LOG("B="+ctx.B.length+" by "+ctx.B[0].length);

							ctx.Save = [ {u: ctx.M}, {u:ctx.M+1}, {u:ctx.M+2} ];
							res(ctx);

							GET.byStep( ctx, function (evs) {
								LOG(evs);
							});
							ME.exec(ctx, "D=A*A'; E=D+D*3; disp(entry); ");
							LOG( "D=", ctx.D, "E=", ctx.E);
						}

					with initial context: 

						{
							"M": 3, 
							"query": {  // default sql-entry query parms if none supplied on url
								"Name": "DefaultTestName"
							},
							"entry": {
								"A": "SELECT a2,a3,a6 FROM MATtest WHERE least(?,1)",
								"B": "SELECT a1,a6 FROM MATtest WHERE least(?,1)"
							},
							"exit": {
								"A": "INSERT INTO ?? SET ?"
							}
						}

					will, on entry, prime its *A* and *B* context keys using its *entry* sql-queries: the "?" therein
					references its context *query* hash (as overridden by url query parameters).  On
					exit, its context *B* context variable is exported by its *exit* sql-query.    It computes a *tau* variable
					that is simply discarded on exit (placing into CTX.Save to retain).  The ME.exec matlab emulator
					computes context *D* and *E* values.  The default ME.exec emulator has been extended with *disp* etc 
					functions.

					This [stateless js-engine example](/jsdemo2.exe?name=test):

						function jsdemo2(ctx,res) {
							ME.exec(ctx, "a = inv(X' * X) * X' * y");
							
							LOG(ctx.a);

							ctx.N = X.length;
							var b = {Name:Name,Tests:N,Computed:new Date()}, a=scope.a;
							for (var n=0,N=a.length;n<N;n++) b[n] = a[n];

							LOG(b);
							res({a:ctx.a,b:b});							
						}

					with initial context: 
					
						{						
							"M": 3,
							"entry": {
								"X": "SELECT p0,p1,p2 FROM Htest WHERE least(?,1)",
								"y": "SELECT FPR from Htest WHERE least(?,1)"
							}
						}

					uses the mathjs MATH module to do regression analysis via an emulated Matlab machine.  Here, 
					the **Name**, **Used** and **M** parameters -- acquired from the URL and/or context query -- are 
					used to retrieve data 
					from the **Htest** dataset.  This data is then used to setup a regression companion 
					matrix **X** and measurement vector **y**.  Regression results **a** are then saved into 
					a **b** vector (which, for example, may be saved with a Context.entry sql).

					Whereas the previous engines are stateless -- they do not define functions (i.e. ports) -- this
					[stateful js-engine](/jsdemo3.exe):

							x = 123;
							function fi(tau,parm) {
							  return 0;
							}; 

							function fo(tau,parm) {
							  return 0;
							};
							
							function jsdemo3(ctx,res) {
							}

					with initial context: 
					
						{ 
							"fi" : {x:10, y:20},
							"fo": {x:11, y:21} 
						}

					defines **fi**, **fo** in its context, as [demonstrated here](/demo3.js).
					
					Subsequent requests at the step/POST endpoint with **port** = "fi" or "fo" will call the
					function with the current **tau** workflow events and the **parm** set to the 
					corresponding "ports" hash.
					
					The following [multi tasking example](/jsdemo4.exe) illustrates how to shard tasks 
					among workers and nodes:
					
						TASK({  
							keys: "i,j,k",  	// e.g. array indecies
							i: [0,1,2,3],  		// domain of index i
							j: [4,8],				// domain of index j
							k: [0],					// domain of index k
							qos: 0,				// regulation time in ms if not zero
							workers: 4, 		// limit number of workers (aka cores) per node
							nodes: 3 			// limit number of nodes (ala locales) in the cluster
						}, 
							// here, a simple task that returns a message 
							($) => "my result is " + (i + j*k) + " from " + $.worker + " on "  + $.node,
							
							// here, a simple callback that displays the task results
							(msg) => console.log(msg) 
						);

	#accordion.Machines
		:markdown
			Engines employ a MAC = opencv | python | ... machine implemented under
			**engine/ifs/MAC/MAC.cpp**, these machines being bound to *#{title}* using the 
			*node-gyp rebuild* provided by *maint.sh bind*.  
			
			All opencv-machines, for example, implements the following pattern:

				class OPORT { 								 	// output port
					OPORT(str Name, V8OBJECT Parm) { 		// Initiallize 
					};
					// Members follow
				};
				class IPORT { 								 	// input port
					IPORT(str Name, V8OBJECT Parm) { 		// Initiallize 
					};
					// Members follow						
				};
				class FEATURE { 							// Output object
					FEATURE( ... ) {						// Initialize 
					};	
					// Members follow
				};
				class CVMACHINE : public MACHINE {  
					int atch(IPORT &port, V8ARRAY tau) { 	// Latch input context to input port
						return 0; // if successful
					}
					int latch(V8ARRAY tau, OPORT &port) { 	// Latch output port to output context
						return 0; // if successful
					}
					int program (void) { 		// program and step machine
					}
					int call(const V8STACK& args) {  // nodejs interface
					}
					// Members follow
				}

			When bound to *#{title}* (using *node-gyp rebuild* provided by *maint.sh bind*), a **pool** of 
			(typically 256) MAC machines is reserved to run multiple (context independent) compute 
			threads at

				error = MAC.call( [ id string, code string, context hash ] )
				error = MAC.call( [ id string, port string, context hash or event list] )

			this call returning an interger error code (non-zero if a fault occured).

			The thread id (typically "Name.Client.Instance") uniquely identifies the compute thread.  
			Compute threads can be freely added to the pool until the pool becomes full.  

			When stepping a machine, the code string specifies either the name of the engine port on 
			which the arriving context is latched, or the name of the output port on which the departing 
			context is latched; thus stepping the machine in a stateful way (to maximize data restfulness).
			Given, however, an empty code string , the machine is stepped in a stateless way, that is, 
			by latching context to all input ports, then latching all output ports to the context.

	#accordion.Commands
		:markdown
			The following commands maintain *#{title}*:

				GET /ping	check client-server connectivity
				GET /alert	broadcast alert &msg to all clients
				GET /stop	stops the server with alert &msg broadcasted to all clients
				GET /bit	built-in test with &N client connections at rate &lambda=events/s
				GET /service/algorithm/ENGINE Execute ENGINE with SOAP/XML parameters
				GET /riddle	validate client when antibot protection configured

//- UNCLASSIFIED